{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, requests, threading, time, tldextract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wikipedia as wp\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuring matplotlib to output opaque images to avoid issues with dark mode\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"savefig.facecolor\"] = \"w\"\n",
    "\n",
    "# We do only read operations, therefore no user config is necessary.\n",
    "# Normally the system crashes when there is no user config unless we tell it otherwise with this environment variable.\n",
    "#   0 is default\n",
    "#   1 means ignore the config\n",
    "#   2 means ignore the config and don't throw warnings\n",
    "os.environ[\"PYWIKIBOT_NO_USER_CONFIG\"] = \"2\"\n",
    "\n",
    "# Now we can import pywikibot\n",
    "import pywikibot as pwb\n",
    "import pywikibot.data.api as api\n",
    "\n",
    "# Then we can setup references for Wikipedia and Wikidata\n",
    "wiki_site = pwb.Site(code=\"en\", fam=\"wikipedia\")\n",
    "data_site = pwb.Site(code=\"wikidata\", fam=\"wikidata\")\n",
    "repo = data_site.data_repository()\n",
    "\n",
    "# Setting important dates\n",
    "stime = \"2015070100\"\n",
    "etime = str(datetime.today().date()).replace(\"-\", \"\") + \"00\"\n",
    "\n",
    "# We list here the search terms for EPFL\n",
    "epfl_alts = [\n",
    "    \"EPFL\",\n",
    "    \"École Polytechnique Fédérale de Lausanne\",\n",
    "    \"Swiss Federal Institute of Technology\",\n",
    "    \"EPF Lausanne\",\n",
    "    \"ETH Lausanne\",\n",
    "    \"Poly Lausanne\",\n",
    "]\n",
    "\n",
    "# URL and headers for pageview website\n",
    "pv_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/%s/daily/%s/%s\"\n",
    "pv_head = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"}\n",
    "\n",
    "# Setting paths\n",
    "im_path = \"graphs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a list of all the pages that mention EPFL. We set `namespace=0` as this namespace is the one that contains regular pages. Note that searches from Pywikibot are ordered alphabetically by default, unlike what the Wikipedia API does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pagenames(pnames):\n",
    "    with open(\"pagenames.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(pnames), )\n",
    "        \n",
    "def load_pagenames():\n",
    "    try:\n",
    "        with open(\"pagenames.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().split(\"\\n\")\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def collect_pages(site, keywords):\n",
    "    # Search for each keyword\n",
    "    searches = [site.search(kw.lower(), where=\"text\", namespaces=0) for kw in keywords]\n",
    "    \n",
    "    # Get the pages and flatten\n",
    "    pages_raw = [list(s) for s in searches]\n",
    "    pages = list(set([item for sublist in pages_raw for item in sublist]))\n",
    "    ret = {}\n",
    "    old_names = [] # load_pagenames()\n",
    "    ignored = []\n",
    "    \n",
    "    # We filter out pages that do not have a WikiData item\n",
    "    for p in tqdm(pages):\n",
    "        try:\n",
    "            if p.title() not in old_names:\n",
    "                x = p.data_item().title()\n",
    "        except:\n",
    "            ignored.append(p.title())\n",
    "            continue\n",
    "\n",
    "        ret[x] = p\n",
    "    \n",
    "    new_pnames = list(set(old_names + [p.title() for p in ret]))\n",
    "    save_pagenames(new_pnames)\n",
    "    print(ignored)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████                                                                         | 385/5001 [02:21<25:47,  2.98it/s]WARNING: wikibase-sense datatype is not supported yet.\n",
      "WARNING: wikibase-sense datatype is not supported yet.\n",
      "WARNING: wikibase-sense datatype is not supported yet.\n",
      " 27%|████████████████████▉                                                         | 1343/5001 [08:08<24:27,  2.49it/s]WARNING: wikibase-form datatype is not supported yet.\n",
      " 55%|███████████████████████████████████████████▏                                  | 2768/5001 [16:59<12:57,  2.87it/s]WARNING: wikibase-form datatype is not supported yet.\n",
      " 64%|█████████████████████████████████████████████████▊                            | 3191/5001 [19:41<10:33,  2.86it/s]WARNING: wikibase-lexeme datatype is not supported yet.\n",
      " 65%|██████████████████████████████████████████████████▉                           | 3264/5001 [20:12<08:45,  3.30it/s]WARNING: wikibase-lexeme datatype is not supported yet.\n",
      " 65%|██████████████████████████████████████████████████▉                           | 3269/5001 [20:16<16:32,  1.75it/s]WARNING: wikibase-lexeme datatype is not supported yet.\n",
      " 95%|█████████████████████████████████████████████████████████████████████████▉    | 4741/5001 [29:33<01:42,  2.52it/s]WARNING: wikibase-sense datatype is not supported yet.\n",
      " 96%|███████████████████████████████████████████████████████████████████████████▏  | 4822/5001 [30:07<02:03,  1.45it/s]WARNING: wikibase-lexeme datatype is not supported yet.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████▋| 4984/5001 [31:05<00:07,  2.36it/s]WARNING: wikibase-form datatype is not supported yet.\n",
      "WARNING: wikibase-lexeme datatype is not supported yet.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5001/5001 [31:12<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Niko Geldner', 'Eastern Pennsylvania Football League', 'Kristin Schirmer']\n",
      "4998 pages currently tracked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epfl_pages = collect_pages(wiki_site, epfl_alts)\n",
    "print(len(epfl_pages), \"pages currently tracked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that Wikipedia started storing page view statistics since July 1st, 2015. This means we will not have any data available before that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Q696817', Page('Mediterranean and Middle East theatre of World War II')),\n",
       " ('Q2908764', Page('Tomorrowland Transit Authority PeopleMover')),\n",
       " ('Q29266709', Page('Sandro Carrara')),\n",
       " ('Q22691484', Page('Annalisa Buffa')),\n",
       " ('Q790012', Page('Out-of-body experience')),\n",
       " ('Q6641499', Page('List of stories set in a future now past')),\n",
       " ('Q2208719', Page('Imee Marcos')),\n",
       " ('Q5347504', Page('Effluent sewer')),\n",
       " ('Q7892', Page('Drinking water')),\n",
       " ('Q7222877', Page('Grading systems by country'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(epfl_pages.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing a page\n",
    "\n",
    "Let's look at the different ways we can refer to a given page. We will be using Martin Vetterli's page for our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Page('Martin Vetterli')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get a page by name\n",
    "page = pwb.Page(wiki_site, u\"Martin Vetterli\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemPage('Q6776811')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get its data reference\n",
    "item = page.data_item()\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemPage('Q6776811')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the reference directly\n",
    "item = pwb.ItemPage(repo, \"Q6776811\")\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonswiki\n",
      "\tCategory:Martin Vetterli\n",
      "dewiki\n",
      "\tMartin Vetterli\n",
      "enwiki\n",
      "\tMartin Vetterli\n",
      "frwiki\n",
      "\tMartin Vetterli\n",
      "frwikiquote\n",
      "\tMartin Vetterli\n"
     ]
    }
   ],
   "source": [
    "# And we can get all the pages linked to this reference through WikiData\n",
    "for k, v in dict(item.sitelinks).items():\n",
    "    print(k + \"\\n\\t\" + v.ns_title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the warning says, the default API request gives us a maximum of 60 days of data. This is insufficient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: API warning (pageviews): The value \"1000\" for parameter \"pvipdays\" must be between 1 and 60.\n"
     ]
    }
   ],
   "source": [
    "req = api.Request(site=data_site, parameters={'action': 'query',\n",
    "                                                'titles': item,\n",
    "                                                'prop': 'pageviews',\n",
    "                                                'pvipdays': 1000})\n",
    "\n",
    "print(\"As the warning says, the default API request gives us a maximum of\", len(req.submit()['query']['pages'][str(item.pageid)]['pageviews']), \"days of data. This is insufficient.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digging deeper into the API, we see we have access to 2063 days of pageview data since July 1, 2015 as of February 22, 2021.\n"
     ]
    }
   ],
   "source": [
    "# Set dates\n",
    "pp_first = datetime.strptime(stime, \"%Y%m%d00\").strftime(\"%B %d, %Y\").replace(\" 0\", \" \")\n",
    "pp_today = datetime.today().strftime(\"%B %d, %Y\").replace(\" 0\", \" \")\n",
    "\n",
    "# Request data\n",
    "page_name = \"Martin_Vetterli\"\n",
    "r = requests.get(pv_url % (page_name, stime, etime), headers=pv_head)\n",
    "print(\"Digging deeper into the API, we see we have access to\", len(r.json()['items']), \"days of pageview data since\", pp_first, \"as of\", pp_today + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentions of EPFL in a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagecounts(page, strings):\n",
    "    return [page.text.count(s) for s in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EPFL': 12,\n",
       " 'École Polytechnique Fédérale de Lausanne': 6,\n",
       " 'Swiss Federal Institute of Technology': 0,\n",
       " 'EPF Lausanne': 0,\n",
       " 'ETH Lausanne': 0,\n",
       " 'Poly Lausanne': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs = pagecounts(page, epfl_alts)\n",
    "dict(zip(epfl_alts, pcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentions over time\n",
    "\n",
    "For `getPageChanges`, we recover all revisions at once at it is significantly faster to do that than to call `page.getOldVersion` continuously.\n",
    "\n",
    "It is important to note that our process is simplified to improve performance by minimizing the number of revisions requested; if a page had 5 mentions in 2008, 5 in 2010, but 4 in 2009, the 4 will be glossed over as we assume the amount of such cases will be rare and insignificant.\n",
    "\n",
    "Getting the revisions without the content is approximately 7 times faster on large pages (35 vs. 5 seconds), while getting the text from the revisions or from `getOldVersion` takes the same amount of time. However we are still getting faster results for revisions with content when it is part of our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class BackgroundGenerator(threading.Thread):\n",
    "    def __init__(self, generator):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = Queue.Queue(1)\n",
    "        self.generator = generator\n",
    "        self.daemon = True\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        for item in self.generator:\n",
    "            self.queue.put(item)\n",
    "        self.queue.put(None)\n",
    "\n",
    "    def next(self):\n",
    "            next_item = self.queue.get()\n",
    "            if next_item is None:\n",
    "                 raise StopIteration\n",
    "            return next_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gen = page.revisions(content=True)\n",
    "check = True\n",
    "start = time.time()\n",
    "while check:\n",
    "    try:\n",
    "        n = next(gen)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "\n",
    "gen = page.revisions(content=True)\n",
    "start = time.time()\n",
    "list(gen)\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(page.latest_revision.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5911023616790771\n",
      "0.6535177230834961\n"
     ]
    }
   ],
   "source": [
    "page.revision_count()\n",
    "\n",
    "start = time.time()\n",
    "list(page.revisions())\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "[x for x in page.revisions()]\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMentionCounts(rev, strings):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(revs, strings, idx):\n",
    "    page = revs[idx]\n",
    "    return sum([page.text.count(s) for s in strings])\n",
    "\n",
    "def getOrUpdate(revs, strings, counts, idx, changes):\n",
    "    if idx not in counts:\n",
    "        temp = getCounts(revs, strings, idx)\n",
    "        counts[idx] = temp\n",
    "        \n",
    "        # Do not consider the count if an earlier revision had more\n",
    "        if not any([counts[k] > temp for k in counts.keys() if k < idx]):\n",
    "            changes[temp] = min(changes.get(temp) or idx, idx) \n",
    "    \n",
    "    return counts[idx]\n",
    "\n",
    "def getMentions(revs, strings, code):\n",
    "    if not getCounts(revs, strings, 0):\n",
    "        return None\n",
    "    \n",
    "    revs = list(page.revisions(reverse=True, content=True))\n",
    "\n",
    "    # Start with whole scope\n",
    "    queue = [(0, len(revs) - 1)]\n",
    "    \n",
    "    # To avoid double checking revisions we store the counts here\n",
    "    cnts = {}\n",
    "    \n",
    "    # And here we store the count-index pairs\n",
    "    changes = {}\n",
    "\n",
    "    while queue:\n",
    "        # Process first element\n",
    "        r0, r1 = queue[0]\n",
    "        queue = queue[1:]\n",
    "\n",
    "        # Only proceed if current scope covers multiple indices\n",
    "        if r0 != r1:\n",
    "            # Get counts for both indices\n",
    "            v0 = getOrUpdate(revs, strings, cnts, r0, changes)\n",
    "            v1 = getOrUpdate(revs, strings, cnts, r1, changes)\n",
    "\n",
    "            # Only proceed if there is a change of count in the current scope\n",
    "            if v0 != v1 and abs(r1 - r0) > 1:\n",
    "                mid = (r0 + r1) // 2\n",
    "                queue.extend([(r0, mid), (mid, r1)])\n",
    "\n",
    "    changes = {revs[v][\"timestamp\"]: k for k, v in changes.items()}\n",
    "    changes = {datetime.combine(k.date(), k.time()): v for k, v in changes.items()}\n",
    "    \n",
    "    # Here we simplify our data to a maximum of one point per month (we take the last one)\n",
    "    changes = pd.Series(changes, name=\"Mentions\").sort_index().groupby(pd.Grouper(freq=\"1M\")).nth(-1)\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pms = getMentions(page, epfl_alts)\n",
    "pms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pms.plot(linestyle='--', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORARY, NEED TO ABSTRACT THE ARRAY CREATION PROCESS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q4909123</th>\n",
       "      <th>Q479473</th>\n",
       "      <th>Q4909123</th>\n",
       "      <th>Q479473</th>\n",
       "      <th>Q1371037</th>\n",
       "      <th>Q5570251</th>\n",
       "      <th>Q4357330</th>\n",
       "      <th>Q6379615</th>\n",
       "      <th>Q116932</th>\n",
       "      <th>Q87732708</th>\n",
       "      <th>...</th>\n",
       "      <th>Q50785019</th>\n",
       "      <th>Q1526025</th>\n",
       "      <th>Q6603256</th>\n",
       "      <th>Q3369179</th>\n",
       "      <th>Q3045752</th>\n",
       "      <th>Q125728</th>\n",
       "      <th>Q2345395</th>\n",
       "      <th>Q119670</th>\n",
       "      <th>Q1736506</th>\n",
       "      <th>Q7526</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-04-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-05-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-06-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-08-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Q4909123  Q479473  Q4909123  Q479473  Q1371037  Q5570251  \\\n",
       "2002-04-01       NaN      NaN       NaN      NaN       NaN       NaN   \n",
       "2002-05-01       NaN      NaN       NaN      NaN       NaN       NaN   \n",
       "2002-06-01       NaN      NaN       NaN      NaN       NaN       NaN   \n",
       "2002-07-01       NaN      NaN       NaN      NaN       NaN       NaN   \n",
       "2002-08-01       NaN      NaN       NaN      NaN       NaN       NaN   \n",
       "...              ...      ...       ...      ...       ...       ...   \n",
       "2020-11-01       1.0      NaN       1.0      NaN       2.0       4.0   \n",
       "2020-12-01       1.0      NaN       1.0      NaN       2.0       4.0   \n",
       "2021-01-01       1.0      NaN       1.0      NaN       2.0       4.0   \n",
       "2021-02-01       1.0      1.0       1.0      1.0       2.0       4.0   \n",
       "2021-03-01       1.0      1.0       1.0      1.0       2.0       4.0   \n",
       "\n",
       "            Q4357330  Q6379615  Q116932  Q87732708  ...  Q50785019  Q1526025  \\\n",
       "2002-04-01       NaN       NaN      NaN        NaN  ...        NaN       NaN   \n",
       "2002-05-01       NaN       NaN      NaN        NaN  ...        NaN       NaN   \n",
       "2002-06-01       NaN       NaN      NaN        NaN  ...        NaN       NaN   \n",
       "2002-07-01       NaN       NaN      NaN        NaN  ...        NaN       NaN   \n",
       "2002-08-01       NaN       NaN      NaN        NaN  ...        NaN       NaN   \n",
       "...              ...       ...      ...        ...  ...        ...       ...   \n",
       "2020-11-01       NaN       1.0      4.0        NaN  ...        3.0       3.0   \n",
       "2020-12-01       3.0       1.0      4.0        NaN  ...        3.0       3.0   \n",
       "2021-01-01       3.0       1.0      4.0        NaN  ...        3.0       3.0   \n",
       "2021-02-01       3.0       1.0      4.0        2.0  ...        3.0       3.0   \n",
       "2021-03-01       3.0       1.0      4.0        2.0  ...        3.0       3.0   \n",
       "\n",
       "            Q6603256  Q3369179  Q3045752  Q125728  Q2345395  Q119670  \\\n",
       "2002-04-01       NaN       NaN       NaN      NaN       NaN      NaN   \n",
       "2002-05-01       NaN       NaN       NaN      NaN       NaN      NaN   \n",
       "2002-06-01       NaN       NaN       NaN      NaN       NaN      NaN   \n",
       "2002-07-01       NaN       NaN       NaN      NaN       NaN      NaN   \n",
       "2002-08-01       NaN       NaN       NaN      NaN       NaN      NaN   \n",
       "...              ...       ...       ...      ...       ...      ...   \n",
       "2020-11-01       1.0      10.0       1.0      2.0       1.0      1.0   \n",
       "2020-12-01       1.0      10.0       1.0      2.0       1.0      1.0   \n",
       "2021-01-01       1.0      10.0       1.0      2.0       1.0      1.0   \n",
       "2021-02-01       1.0      10.0       1.0      2.0       1.0      1.0   \n",
       "2021-03-01       1.0      10.0       1.0      2.0       1.0      1.0   \n",
       "\n",
       "            Q1736506  Q7526  \n",
       "2002-04-01       NaN    NaN  \n",
       "2002-05-01       NaN    NaN  \n",
       "2002-06-01       NaN    NaN  \n",
       "2002-07-01       NaN    NaN  \n",
       "2002-08-01       NaN    NaN  \n",
       "...              ...    ...  \n",
       "2020-11-01       1.0    3.0  \n",
       "2020-12-01       1.0    3.0  \n",
       "2021-01-01       1.0    3.0  \n",
       "2021-02-01       1.0    3.0  \n",
       "2021-03-01       1.0    3.0  \n",
       "\n",
       "[228 rows x 137 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(\"pickles\", \"en_mentions.pkl\")\n",
    "prev = pd.read_pickle(path)\n",
    "prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Martin Vetterli'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "d = {1:2,3:4}\n",
    "\n",
    "for a, b in d.items():\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subGetRevisions(page, ret, minDate=None):\n",
    "    # Need to implement minDate functionality\n",
    "    return list(page.revisions(reverse=False, content=True))\n",
    "\n",
    "def subGetMentions(revs, code, strings, ret):\n",
    "    ret = getMentions(revs, code, strings)\n",
    "\n",
    "def subGetSizes(revs, code, ret):\n",
    "    ret = getSizes(revs)\n",
    "\n",
    "def subGetEdits(revs, code, ret):\n",
    "    pass\n",
    "\n",
    "def subGetViews(page, code, ret):\n",
    "    ret = getViews(page)\n",
    "    \n",
    "def getSizes(revs, code):\n",
    "    df = pd.DataFrame([dict(r) for r in revs])\n",
    "    df = df[[\"userid\", \"timestamp\", \"size\"]]\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Get absolute size from relative size\n",
    "    df[\"diff\"] = (df['size'] - df['size'].shift(1)).abs()\n",
    "    df[\"diff\"] = df[\"diff\"].fillna(df[\"size\"])\n",
    "    \n",
    "    # Sample every month and shift by 1 day to get 1st of month\n",
    "    se = df[\"size\"].groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "    se.index = se.index.shift(1, freq=\"D\")\n",
    "    \n",
    "    return se.rename(code)\n",
    "\n",
    "# Need to make it wiki-independant\n",
    "def getViews(page):\n",
    "    r = requests.get(pv_url % (page.title(), stime, etime), headers=pv_head)\n",
    "    se = pd.Series({datetime.strptime(str(item['timestamp'])[:-2], \"%Y%m%d\"): item['views'] for item in req.json()['items']}, name=\"Views\")\n",
    "    return se.rename(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-50-af26ba9f3dfd>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-50-af26ba9f3dfd>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    get revs of next and process curr\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def updateTimeSeries(pagecodes, strings, rescan=False, flush=False):\n",
    "    path = None if flush else os.path.join(\"pickles\", \"en_mentions.pkl\")\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_pickle(path)\n",
    "    except:\n",
    "        data = pd.DataFrame()\n",
    "        \n",
    "    dfs = [prev]\n",
    "    \n",
    "    if rescan:\n",
    "        pagecodes += list(prev.columns)\n",
    "\n",
    "    last_ts = data.index[-1]\n",
    "    prev_code = None\n",
    "    \n",
    "    for curr_code, curr_page in tqdm(pages.items()):\n",
    "        # Not needed for now : prev[\"Q7526\"].last_valid_index()\n",
    "        # later we can see about getting only part of the revisions\n",
    "        \n",
    "        revs, pmns, pszs, peds, pvws = None, None, None, None, None\n",
    "        prev_code = curr_code\n",
    "        \n",
    "        t_revs = threading.Thread(target=getRevisions, args=(curr_page, revs,))\n",
    "        t_revs.start()\n",
    "        \n",
    "        if prev_code:\n",
    "            t_pmns = threading.Thread(target=subGetMentions, args=(revs, prev_code, strings, pmns,))\n",
    "            t_pszs = threading.Thread(target=subGetSizes, args=(revs, prev_code, pszs,))\n",
    "            t_peds = threading.Thread(target=subGetEdits, args=(revs, prev_code, peds,))\n",
    "            t_pvws = threading.Thread(target=subGetViews, args=(revs, prev_code, pvws,))\n",
    "            \n",
    "            t_pmns.start()\n",
    "            t_pszs.start()\n",
    "            t_peds.start()\n",
    "            t_pvws.start()\n",
    "            \n",
    "            t_pmns.join()\n",
    "            t_pszs.join()\n",
    "            t_peds.join()\n",
    "            t_pvws.join()\n",
    "            \n",
    "        t_revs.join()\n",
    "\n",
    "        # Set limit timestamp (or None if no data yet)\n",
    "        ts = prev[code].last_valid_index() if code in prev.columns else None\n",
    "        \n",
    "        # Get values after that timestamp\n",
    "        df = getMentions(p, epfl_alts)\n",
    "        \n",
    "        if df is not None and len(df):\n",
    "            df.name = code\n",
    "            df = df.groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "            df.index = df.index.shift(1, freq=\"D\")\n",
    "            \n",
    "            # Combine with old data if it exists\n",
    "            if code in prev.columns:\n",
    "                df = df.combine_first(prev[code])\n",
    "\n",
    "            dfs.append(df)\n",
    "    \n",
    "    curr = pd.concat(dfs, axis=1)\n",
    "    curr = curr.ffill(axis=0)\n",
    "    curr.to_pickle(path)\n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages[:10]\n",
    "pcodes = [p.data_item().title() for p in epfl_pages[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_mentions = updateMentions(pcodes[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mtns = bulk_mentions.sum(axis=1)\n",
    "total_mtns.index.name = \"Date\"\n",
    "total_mtns.name = \"Total mentions\"\n",
    "total_mtns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = total_mtns.plot(legend=True, title=\"Mentions of EPFL on Wikipedia\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Mentions')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"mentions.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED TO MAKE THE PCODES FASTER\n",
    "\n",
    "Par exemple en mémorisant les codes des pages  \n",
    "Stocker les bulk en sparse matrix avec des valeurs uniquement la ou ca change et pour generer le graphe on fait juste un pulldown  \n",
    "Sauver régulièrement, mettre un intervalle par défaut  \n",
    "PARALLELIZE  \n",
    "Si la reference existe deja, aller avec des previous revisions au lieu d'appeler revisions (faire un test pour voir la différence de performance), on assume que les collectes de données se font régulièrement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views of a page over time\n",
    "\n",
    "Here we compute the pageviews for each day and highlight the outliers.\n",
    "In the future we will compute outliers based on the local average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageviews(page, stime, etime):\n",
    "    req = requests.get(pv_url % (page.title(), stime, etime))\n",
    "    serie = pd.Series({datetime.strptime(str(item['timestamp'])[:-2], \"%Y%m%d\"): item['views'] for item in req.json()['items']}, name=\"Views\")\n",
    "    return serie\n",
    "\n",
    "def outliers(serie):\n",
    "    return pd.Series(serie[np.abs(stats.zscore(serie)) > 3], name=\"Outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs = pageviews(page, stime, etime)\n",
    "pvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = outliers(pvs)\n",
    "ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pvs.plot(legend=True, title=\"Daily views for 'Martin Vetterli' on Wikipedia\")\n",
    "ax = ols.plot(legend=True, ax=ax, linestyle=\"\", marker=\"o\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Views')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "\n",
    "for i, each in enumerate(ols.index):\n",
    "    y = ols[each]\n",
    "    ax.text(each + timedelta(25), y, y)\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"views.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, let's generate a list of all the pages that mention EPFL. We set `namespace=0` as this namespace is the one that contains regular pages. Note that searches from Pywikibot are ordered alphabetically by default, unlike what the Wikipedia API does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs2 = pageviews(page, stime, etime)\n",
    "to_rem = ols.index\n",
    "pvs2[to_rem] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be good to cover the whole dataset\n",
    "\n",
    "v = 100\n",
    "ax = pvs2.plot(legend=True)\n",
    "(pvs2.rolling(v, center=True).sum() / v).plot(ax=ax, legend=True, label=\"100-day average\", title=\"Daily views for 'Martin Vetterli' on Wikipedia, 100-day average without outliers\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Views')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"views__avg.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlinks\n",
    "\n",
    "So far I haven't found an efficient way to account for backlinks in page revisions. Therefore this will be skipped for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(page.revisions(reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageSize(page):\n",
    "    revs = list(page.revisions(reverse=True))\n",
    "    df = pd.DataFrame([dict(r) for r in revs])\n",
    "    df = df[[\"userid\", \"timestamp\", \"size\", \"minor\"]]\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Get absolute size from relative size\n",
    "    df[\"diff\"] = (df['size'] - df['size'].shift(1)).abs()\n",
    "    df[\"diff\"] = df[\"diff\"].fillna(df[\"size\"])\n",
    "    \n",
    "    # Sample every month and shift by 1 day to get 1st of month\n",
    "    df = df[\"size\"].groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "    df.index = df.index.shift(1, freq=\"D\")\n",
    "    \n",
    "    return df.rename(page.data_item().title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2007-09-01     1441\n",
       "2007-10-01     1439\n",
       "2007-11-01     1439\n",
       "2007-12-01     1439\n",
       "2008-01-01     1439\n",
       "              ...  \n",
       "2020-11-01    11895\n",
       "2020-12-01    11843\n",
       "2021-01-01    11839\n",
       "2021-02-01    11839\n",
       "2021-03-01    11840\n",
       "Name: Q6776811, Length: 163, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psz = getPageSize(page)\n",
    "psz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = psz.plot()\n",
    "ax.set_title(\"Size of Martin Vetterli's page\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Size in Bytes\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"mirko.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageScore(page):\n",
    "    # Get mentions\n",
    "    pms = getMentions(page, epfl_alts)\n",
    "    \n",
    "    if pms is None:\n",
    "        return None\n",
    "    \n",
    "    # Get page views\n",
    "    pvs = pageviews(page, stime, etime)\n",
    "    \n",
    "    # Get page size\n",
    "    psz = getPageSize(page)\n",
    "    \n",
    "    # Combine the data\n",
    "    df = pd.concat([pvs, psz.reindex(pvs.index), pms.reindex(pvs.index)], axis=1)\n",
    "    df = df.ffill(axis=0)\n",
    "    \n",
    "    s_prev = psz.index.difference(pvs.index)\n",
    "    s_fill = psz[s_prev[-1]] if len(s_prev) else 0\n",
    "    df[\"Size\"] = df[\"Size\"].fillna(s_fill)\n",
    "    \n",
    "    m_prev = pms.index.difference(pvs.index)\n",
    "    m_fill = pms[m_prev[-1]] if len(m_prev) else 0\n",
    "    df[\"Mentions\"] = df[\"Mentions\"].fillna(m_fill)\n",
    "    \n",
    "    # Generate score\n",
    "    df[\"Score\"] = df[\"Mentions\"] * df[\"Views\"] / df[\"Size\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psc = getPageScore(page)\n",
    "psc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editors\n",
    "\n",
    "We will consider a user that changed their name as a new user for simplicity's sake.  \n",
    "We do not use the function `page.contributors()` as it makes no distinction between regular and minor edits, and since we're going through the revisions we might as well extract that information in the process.\n",
    "\n",
    "This can be improved by weighing the edits depending on the size increase of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = list(page.revisions(reverse=True, content=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions to detect IP addresses and Bots.\n",
    "pat_ip = re.compile('^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$|([0-9a-fA-F][0-9a-fA-F]{0,3}:){7}([0-9a-fA-F][0-9a-fA-F]{0,3}){1}')\n",
    "pat_bot = re.compile(r'bot\\b', re.IGNORECASE)\n",
    "\n",
    "# Classify a user by its name into bots, real users, and IPs (unregistered users)\n",
    "def classify_user(name):\n",
    "    if pat_ip.match(name):\n",
    "        return \"IP\"\n",
    "    elif pat_bot.search(name):\n",
    "        return \"Bot\"\n",
    "    else:\n",
    "        return \"Real\"\n",
    "\n",
    "# Get user edit data from a page\n",
    "def users(revs):\n",
    "    # Get usernames and edit type\n",
    "    df = pd.DataFrame([(r[\"user\"], r[\"minor\"]) for r in revs])\n",
    "    \n",
    "    # Set as index and sort \n",
    "    df.index = pd.MultiIndex.from_frame(df)\n",
    "    df = df[[1]].sort_index(axis=0)\n",
    "    \n",
    "    # Group by user and edit type and split into columns\n",
    "    df = pd.DataFrame(df.groupby(level=[0,1]).size())\n",
    "    df = df.unstack(level=1, fill_value=0)\n",
    "    \n",
    "    # Rename columns and drop useless levels\n",
    "    df = df.droplevel(level=0, axis=1)\n",
    "    df.index.name = \"Usernames\"\n",
    "    df.columns = [\"Major\", \"Minor\"]\n",
    "    \n",
    "    # Add user types as first index level\n",
    "    df.index = pd.MultiIndex.from_tuples([classify_user(i), i] for i in df.index)\n",
    "    \n",
    "    return df.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ueds = users(revs)\n",
    "ueds[\"Edits\"] = ueds[\"Minor\"] + ueds[\"Major\"]\n",
    "\n",
    "ax = sns.swarmplot(x=ueds.index.get_level_values(0), y=\"Edits\", data=ueds)\n",
    "ax.set_title(\"Editors for 'Martin Vetterli' on Wikipedia\")\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"edits.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_users(pages):\n",
    "    # We'll want to get the bulk of the data out in the future\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    for p in pages:\n",
    "        curr = list(p.revisions(reverse=True, content=False))\n",
    "        udata = users(curr)\n",
    "        udata[p.title()] = udata[\"Minor\"] + udata[\"Major\"]\n",
    "        temp.append(udata[p.title()])\n",
    "    \n",
    "    return pd.concat(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udata = mass_users(epfl_pages[:100])\n",
    "udata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_sum = udata.sum(axis=1)\n",
    "usr_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_cnt = udata.count(axis=1)\n",
    "usr_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Scatterplot of user edits\")\n",
    "ax.set_xlabel(\"Number of edits\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "x_vals = np.linspace(0, 100)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (120, 95))\n",
    "fig.set_size_inches((9, 6))\n",
    "artists = []\n",
    "plot_names = []\n",
    "for (ns, s), (nc, c) in zip(usr_sum.groupby(level=0), usr_cnt.groupby(level=0)):\n",
    "    artists.append(plt.scatter(s, c))\n",
    "    plot_names.append(ns)\n",
    "ax.legend(artists, plot_names)\n",
    "ax.figure.savefig(os.path.join(im_path, \"users.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (ns, s), (nc, c) in zip(usr_sum.groupby(level=0), usr_cnt.groupby(level=0)):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to make a record of what pages have indeed mentions or not so we can refer to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(page):\n",
    "    ret = getPageScore(page)\n",
    "    \n",
    "    if ret is None:\n",
    "        return None, None\n",
    "    \n",
    "    idx = page.data_item().title()\n",
    "    ret.to_pickle(os.path.join(\"pickles\", idx))\n",
    "    return idx, ret\n",
    "\n",
    "def loadData(idx):\n",
    "    return pd.read_pickle(os.path.join(\"pickles\", idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "fns = os.listdir(\"pickles\")\n",
    "for p in epfl_pages:\n",
    "    try:\n",
    "        if p.data_item().title() not in fns:\n",
    "            saveData(p)\n",
    "            print(p)\n",
    "    except:\n",
    "        pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[en:Whistleblower]]\n",
    "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banane = pwb.Page(wiki_site, u\"Fréquence Banane\")\n",
    "banane_s = getPageScore(banane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banane.data_item().title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banane_s.to_pickle(banane.data_item().title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(banane.data_item().title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(banane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadData(\"Q3090425\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = testdata[0]\n",
    "b = testdata[1]\n",
    "pd.concat([a, b], axis=1, keys=[\"u\", \"v\"]).fillna(0).sum(axis=1, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = [n for n in os.listdir(\"pickles\") if n[0] == \"Q\"]\n",
    "testdata = [loadData(fn) for fn in fns]\n",
    "\n",
    "init = testdata[0]\n",
    "\n",
    "for td in testdata[1:]:\n",
    "    init = pd.concat([init, td], axis=1, keys=[\"l\", \"r\"]).fillna(0).sum(axis=1, level=1)\n",
    "    \n",
    "init.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init[\"Views\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init[\"Mentions\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init[\"Score\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pd.DataFrame(columns=[\"views\", \"mentions\", \"edits\"])\n",
    "cl.loc[495998] = [1, 2, 4]\n",
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(epfl_pages[:10]):\n",
    "    print(i)\n",
    "    getPageScore(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid, dat in df.groupby(\"userid\"):\n",
    "    print(uid)\n",
    "    for val, x in dat.groupby(\"minor\"):\n",
    "        print(val, sum(x[\"diff\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entry over time\n",
    "* list of incoming links\n",
    "* list of outgoing links\n",
    "* mention history\n",
    "\n",
    "work with user pages  \n",
    "potentially use the wikipedia package for smaller operations as it seems to be faster, maybe do some timed tests\n",
    "\n",
    "count improve counts by checking revisions 5 before and 5 after to ignore edit wars and such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to handle the problem or redirects and page name changes in the future.\n",
    "\n",
    "Will need to classify whether we found the subject through keywords or if EPFL was mentioned.\n",
    "\n",
    "We assume mentions are only added for performance and simplicity reasons. It's very rare that content will be removed from pages.\n",
    "\n",
    "Link pages with contributors\n",
    "\n",
    "Update data over time instead of recomputing everything\n",
    "\n",
    "Bot that changes names of EPFL or makes a suggestion on the talk page\n",
    "\n",
    "orcid, unique identifier for papers\n",
    "\n",
    "https://go.epfl.ch/wikiproject\n",
    "\n",
    "correlate growth of epfl mentions and wikipedia in general (or science related pages, lets see what we can do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r[\"size\"] for r in revs][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talkpage = pwb.Page(wiki_site, u\"Talk:Martin Vetterli\")\n",
    "talkpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(talkpage.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(page.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats[0].categoryinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that checks that the category records are correct (if a page has been removed from a category, it will put it in the legacy records instead)\n",
    "def sanitizeCategories():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extlink_data(page):\n",
    "    links = [link.split(\"/\", 5)[-1] if \"web.archive.org\" in link else link for link in page.extlinks()]\n",
    "    domains = pd.DataFrame([tldextract.extract(link) for link in links], columns=[\"subdomain\", \"domain\", \"suffix\"])\n",
    "    \n",
    "    try:\n",
    "        domains[\"tld\"] = domains[\"suffix\"].str.split(\".\", expand=False).apply(lambda e : e[-1])\n",
    "        domains[\"site\"] = domains[\"domain\"] + \".\" + domains[\"suffix\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sites = domains[\"site\"].value_counts()\n",
    "    tlds = domains[\"tld\"].value_counts()\n",
    "    return page.title(), sites, tlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, sites, tlds = extlink_data(page)\n",
    "tlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = epfl_pages[:500]\n",
    "\n",
    "series = np.array([extlink_data(t) for t in temp], dtype=object).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(s, name=t) for t, s in tuple(series[0:2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = pd.concat(series[1], axis=1, keys=series[0])\n",
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sum = refs.sum(axis=1)\n",
    "ref_sum[ref_sum > 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_cnt = refs.count(axis=1)\n",
    "ref_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(ref_sum, ref_cnt).axes\n",
    "ax.set_title(\"Scatterplot of domain repartition and frequency\")\n",
    "ax.set_xlabel(\"Number of occurences\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((9, 6))\n",
    "x_vals = np.linspace(0, 300)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (320, 280))\n",
    "for t in [\"epfl.ch\", \"unil.ch\", \"bbc.co.uk\", \"google.com\", \"nytimes.com\"]:\n",
    "    ax.scatter(ref_sum[t], ref_cnt[t], color=\"orange\")\n",
    "    ax.annotate(t, (ref_sum[t]+50, ref_cnt[t]+5), color=\"orange\", bbox=dict(facecolor='white', boxstyle=\"round,pad=0.3\"))\n",
    "ax.figure.savefig(os.path.join(im_path, \"domains.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_tld = pd.concat(series[2], axis=1, keys=series[0])\n",
    "refs_tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tld_sum = refs_tld.sum(axis=1)\n",
    "ref_tld_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tld_cnt = refs_tld.count(axis=1)\n",
    "ref_tld_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(ref_tld_sum, ref_tld_cnt).axes\n",
    "ax.set_title(\"Scatterplot of top-level-domain repartition and frequency\")\n",
    "ax.set_xlabel(\"Number of occurences\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((9, 6))\n",
    "x_vals = np.linspace(0, 450)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (720, 400))\n",
    "for t in [\"ch\", \"org\", \"com\", \"gov\"]:\n",
    "    ax.scatter(ref_tld_sum[t], ref_tld_cnt[t], color=\"orange\")\n",
    "    ax.annotate(t, (ref_tld_sum[t]+400, ref_tld_cnt[t]+1), color=\"orange\", bbox=dict(facecolor='white', boxstyle=\"round,pad=0.3\"))\n",
    "ax.figure.savefig(os.path.join(im_path, \"tlds.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "temp = pd.concat(series[2], axis=1, keys=series[0])\n",
    "sys.getsizeof(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = temp.astype(pd.SparseDtype(\"int\", np.nan))\n",
    "sys.getsizeof(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pszs = [getPageSize(p) for p in epfl_pages[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(pszs, axis=1)\n",
    "print(sys.getsizeof(df))\n",
    "\n",
    "print(sys.getsizeof(df.astype(pd.SparseDtype(\"int\", np.nan))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make page to item and item to page functions\n",
    "# Heavily bottlenecked by the `revisions` function unfortunately\n",
    "\n",
    "# Gets given keys of all revisions after a given timestamp\n",
    "def getRevisionsTags(page, ts, keys, content=False):\n",
    "    gen = page.revisions(reverse=True, content=content)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            v = next(gen)\n",
    "            t = v[\"timestamp\"]\n",
    "            \n",
    "            if ts is None or t > ts:\n",
    "                yield t, [v[key] for key in keys]\n",
    "            else:\n",
    "                return\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "def updatePageSizes(pagecodes, rescan=False):\n",
    "    path = os.path.join(\"pickles\", \"en_page_sizes.pkl\")\n",
    "    \n",
    "    try:\n",
    "        prev = pd.read_pickle(path)\n",
    "    except:\n",
    "        prev = pd.DataFrame()\n",
    "        \n",
    "    dfs = [prev]\n",
    "    \n",
    "    if rescan:\n",
    "        pagecodes += list(prev.columns)\n",
    "    \n",
    "    for code in tqdm(pagecodes):\n",
    "        try:\n",
    "            # Recover page from code\n",
    "            p = pwb.Page(wiki_site, pwb.ItemPage(repo, code).sitelinks[\"enwiki\"].ns_title())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Set limit timestamp (or None if no data yet)\n",
    "        ts = prev[code].last_valid_index() if code in prev.columns else None\n",
    "        \n",
    "        # Get values after that timestamp\n",
    "        values = [(t, v[0]) for t, v in getRevisionsTags(p, ts, [\"size\"])]\n",
    "        \n",
    "        if len(values):\n",
    "            # Sample every month and shift by 1 day to get 1st of month\n",
    "            df = pd.DataFrame(values, columns=[0, code]).set_index(0)\n",
    "            df = df.groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "            df.index = df.index.shift(1, freq=\"D\")\n",
    "            \n",
    "            # Combine with old data if it exists\n",
    "            if code in prev.columns:\n",
    "                df = df.combine_first(prev[code])\n",
    "                \n",
    "\n",
    "            dfs.append(df)\n",
    "    \n",
    "    curr = pd.concat(dfs, axis=1)\n",
    "    curr = curr.ffill(axis=0)\n",
    "    curr.to_pickle(path)\n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcodes = [p.data_item().title() for p in epfl_pages[:300]]\n",
    "pszs = updatePageSizes(pcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pszs = pd.read_pickle(\"pickles/en_page_sizes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pszs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = pszs.sum(axis=1)\n",
    "total_size.index.name = \"Date\"\n",
    "total_size.name = \"Select Pages Size\"\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page count\n",
    "\n",
    "Here we look at how many of our pages exist at a given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageCounts(df):\n",
    "    ret = df.count(axis=1)\n",
    "    ret.index.name = \"Date\"\n",
    "    ret.name = \"Select Pages Count\"\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnts = pageCounts(pszs)\n",
    "pcnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page size and page counts ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(total_size / pcnts).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWikistats(fn, name, exp=0):\n",
    "    path = os.path.join(\"csv\", \"wikistats\", fn)\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    df.index = pd.to_datetime(df[\"month\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").rename(\"Date\")\n",
    "    df = df[\"total.total\"].rename(name) * 10**exp\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_size = loadWikistats(\"size_change_en.csv\", name=\"English Wikipedia Size\", exp=0)\n",
    "wiki_size = wiki_size[17:].cumsum()\n",
    "wiki_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_cnts = loadWikistats(\"pages_en.csv\", name=\"English Wikipedia Count\", exp=0)\n",
    "wiki_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_ratio = pd.concat([wiki_size, wiki_cnts], axis=1).dropna()\n",
    "wiki_ratio = wiki_ratio.iloc[:,0] / wiki_ratio.iloc[:,1]\n",
    "wiki_ratio.name = \"English Wikipedia Ratio\"\n",
    "wiki_ratio.plot()\n",
    "wiki_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_ratio = pd.concat([total_size, pcnts], axis=1).dropna()\n",
    "pages_ratio = pages_ratio.iloc[:,0] / pages_ratio.iloc[:,1]\n",
    "pages_ratio.name = \"Select Pages Ratio\"\n",
    "pages_ratio.plot()\n",
    "pages_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([total_size], axis=1).plot(logy=False, figsize=(9, 6))\n",
    "ax.set_title(\"Size in Bytes\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"size_comp_lin.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([total_size, wiki_size], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Size in Bytes (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"size_comp.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([pcnts, wiki_cnts], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Page counts (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"count_comp.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([pages_ratio, wiki_ratio], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Size / Page Count Ratio (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"ratio_comp.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "page.revisions()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epfl_pages[100].title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only run on one page at a time\n",
    "start = time.time()\n",
    "revs = next(iter(api.PropertyGenerator('revisions', site=wiki_site, parameters={\n",
    "    'titles': 'Martin Vetterli',\n",
    "    'rvprop': 'timestamp|size',\n",
    "})))['revisions']\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize queries to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_data = []\n",
    "for p in epfl_pages[:100]:\n",
    "    mention_data.append(getMentions(p, epfl_alts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki-env",
   "language": "python",
   "name": "wiki-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
