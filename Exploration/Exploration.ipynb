{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, requests, threading, time, tldextract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import wikipedia as wp\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing our custom functions\n",
    "from wiki_workers import *\n",
    "\n",
    "# Configuring matplotlib to output opaque images to avoid issues with dark mode\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"savefig.facecolor\"] = \"w\"\n",
    "\n",
    "# We do only read operations, therefore no user config is necessary.\n",
    "# Normally the system crashes when there is no user config unless we tell it otherwise with this environment variable.\n",
    "#   0 is default\n",
    "#   1 means ignore the config\n",
    "#   2 means ignore the config and don't throw warnings\n",
    "os.environ[\"PYWIKIBOT_NO_USER_CONFIG\"] = \"2\"\n",
    "\n",
    "# Now we can import pywikibot\n",
    "import pywikibot as pwb\n",
    "import pywikibot.data.api as api\n",
    "\n",
    "# Then we can setup references for Wikipedia and Wikidata\n",
    "wiki_site = pwb.Site(code=\"en\", fam=\"wikipedia\")\n",
    "data_site = pwb.Site(code=\"wikidata\", fam=\"wikidata\")\n",
    "repo = data_site.data_repository()\n",
    "\n",
    "# Setting paths\n",
    "im_path = \"graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We list here the search terms for EPFL\n",
    "epfl_alts = [\n",
    "    \"EPFL\",\n",
    "    \"École Polytechnique Fédérale de Lausanne\",\n",
    "    \"Swiss Federal Institute of Technology in Lausanne\",\n",
    "    \"EPF Lausanne\",\n",
    "    \"ETH Lausanne\",\n",
    "    \"Poly Lausanne\",\n",
    "]\n",
    "\n",
    "# And here are some other universities so we can compare data\n",
    "unil_alts = [\n",
    "    \"Unil\",\n",
    "    \"Université de Lausanne\",\n",
    "    \"Uni Lausanne\",\n",
    "    \"University of Lausanne\",\n",
    "    \"Lausanne University\",\n",
    "]\n",
    "\n",
    "ethz_alts = [\n",
    "    \"ETHZ\",\n",
    "    \"EPFZ\",\n",
    "    \"Eidgenössische Technische Hochschule Zürich\",\n",
    "    \"ETH Zurich\",\n",
    "    \"Swiss Federal Institute of Technology in Zurich\",\n",
    "    \"École Polytechnique Fédérale de Lausanne\",\n",
    "    \"Poly Zurich\"\n",
    "]\n",
    "\n",
    "mit_alts = [\n",
    "    \"MIT\",\n",
    "    \"Massachusetts Institute of Technology\",\n",
    "    \"Boston Tech\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a list of all the pages that mention EPFL. We set `namespace=0` as this namespace is the one that contains regular pages. Note that searches from Pywikibot are ordered alphabetically by default, unlike what the Wikipedia API does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages = collect_pages(wiki_site, epfl_alts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that Wikipedia started storing page view statistics since July 1st, 2015. This means we will not have any data available before that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePages(\"EPFL\", epfl_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages = loadPages(\"EPFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []\n",
    "pulled_data = []\n",
    "left = list(epfl_pages.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"Start\")\n",
    "\n",
    "try:\n",
    "    while left:\n",
    "        key, item = left.pop()\n",
    "        pulled_data.append(item.revisions(content=True))\n",
    "        done.append(key)\n",
    "        i += 1\n",
    "        print(\"Processed\", i, end=\"\\r\")\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages_sample = dict([(key, epfl_pages[key]) for key in epfl_pages][40:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages_sample = collect_pages(wiki_site, epfl_alts, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uts = updateTimeSeries(epfl_pages_sample, epfl_alts, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to get extra data but not for initial sweep\n",
    "start = time.time()\n",
    "[temppage.getOldVersion(x) for x in tempseries]\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "temppage.revisions(content=True)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing a page\n",
    "\n",
    "Let's look at the different ways we can refer to a given page. We will be using Martin Vetterli's page for our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get a page by name\n",
    "page = pwb.Page(wiki_site, u\"Martin Vetterli\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get its data reference\n",
    "item = page.data_item()\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the reference directly\n",
    "item = pwb.ItemPage(repo, \"Q6776811\")\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can get all the pages linked to this reference through WikiData\n",
    "for k, v in dict(item.sitelinks).items():\n",
    "    print(k + \"\\n\\t\" + v.ns_title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = api.Request(site=data_site, parameters={'action': 'query',\n",
    "                                                'titles': item,\n",
    "                                                'prop': 'pageviews',\n",
    "                                                'pvipdays': 1000})\n",
    "\n",
    "print(\"As the warning says, the default API request gives us a maximum of\", len(req.submit()['query']['pages'][str(item.pageid)]['pageviews']), \"days of data. This is insufficient.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dates\n",
    "pp_first = datetime.strptime(stime, \"%Y%m%d00\").strftime(\"%B %d, %Y\").replace(\" 0\", \" \")\n",
    "pp_today = datetime.today().strftime(\"%B %d, %Y\").replace(\" 0\", \" \")\n",
    "\n",
    "# Request data\n",
    "page_name = \"Martin_Vetterli\"\n",
    "r = requests.get(pv_url % (page_name, stime, etime), headers=pv_head)\n",
    "print(\"Digging deeper into the API, we see we have access to\", len(r.json()['items']), \"days of pageview data since\", pp_first, \"as of\", pp_today + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentions of EPFL in a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagecounts(page, strings):\n",
    "    return [page.text.count(s) for s in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pagecounts(page, epfl_alts)\n",
    "dict(zip(epfl_alts, pcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentions over time\n",
    "\n",
    "For `getPageChanges`, we recover all revisions at once at it is significantly faster to do that than to call `page.getOldVersion` continuously.\n",
    "\n",
    "It is important to note that our process is simplified to improve performance by minimizing the number of revisions requested; if a page had 5 mentions in 2008, 5 in 2010, but 4 in 2009, the 4 will be glossed over as we assume the amount of such cases will be rare and insignificant.\n",
    "\n",
    "Getting the revisions without the content is approximately 7 times faster on large pages (35 vs. 5 seconds), while getting the text from the revisions or from `getOldVersion` takes the same amount of time. However we are still getting faster results for revisions with content when it is part of our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.revision_count()\n",
    "\n",
    "start = time.time()\n",
    "list(page.revisions())\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "[x for x in page.revisions()]\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMentionCounts(rev, strings):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(revs, strings, idx):\n",
    "    text = revs[idx]\n",
    "    return sum([text.text.count(s) for s in strings])\n",
    "\n",
    "def getOrUpdate(revs, strings, counts, idx, changes):\n",
    "    if idx not in counts:\n",
    "        temp = getCounts(revs, strings, idx)\n",
    "        counts[idx] = temp\n",
    "        \n",
    "        # Do not consider the count if an earlier revision had more\n",
    "        if not any([counts[k] > temp for k in counts.keys() if k < idx]):\n",
    "            changes[temp] = min(changes.get(temp) or idx, idx) \n",
    "    \n",
    "    return counts[idx]\n",
    "\n",
    "def getMentions(revs_flip, strings, code):\n",
    "    if not getCounts(revs_flip, strings, 0):\n",
    "        return None\n",
    "    \n",
    "    # Reversing revisions\n",
    "    revs = revs_flip[::-1]\n",
    "\n",
    "    # Start with whole scope\n",
    "    queue = [(0, len(revs) - 1)]\n",
    "    \n",
    "    # To avoid double checking revisions we store the counts here\n",
    "    cnts = {}\n",
    "    \n",
    "    # And here we store the count-index pairs\n",
    "    changes = {}\n",
    "\n",
    "    while queue:\n",
    "        # Process first element\n",
    "        r0, r1 = queue[0]\n",
    "        queue = queue[1:]\n",
    "\n",
    "        # Only proceed if current scope covers multiple indices\n",
    "        if r0 != r1:\n",
    "            # Get counts for both indices\n",
    "            v0 = getOrUpdate(revs, strings, cnts, r0, changes)\n",
    "            v1 = getOrUpdate(revs, strings, cnts, r1, changes)\n",
    "\n",
    "            # Only proceed if there is a change of count in the current scope\n",
    "            if v0 != v1 and abs(r1 - r0) > 1:\n",
    "                mid = (r0 + r1) // 2\n",
    "                queue.extend([(r0, mid), (mid, r1)])\n",
    "\n",
    "    changes = {revs[v][\"timestamp\"]: k for k, v in changes.items()}\n",
    "    changes = {datetime.combine(k.date(), k.time()): v for k, v in changes.items()}\n",
    "    \n",
    "    # Here we simplify our data to a maximum of one point per month (we take the last one)\n",
    "    changes = pd.Series(changes, name=\"Mentions\").sort_index().groupby(pd.Grouper(freq=\"1M\")).nth(-1)\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pms = getMentions(page, epfl_alts)\n",
    "pms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pms.plot(linestyle='--', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"pickles\", \"en_mentions.pkl\")\n",
    "prev = pd.read_pickle(path)\n",
    "prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1:2,3:4}\n",
    "\n",
    "for a, b in d.items():\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = list(page.revisions(reverse=False, content=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subGetRevisions(page, ret, minDate=None):\n",
    "    # Need to implement minDate functionality\n",
    "    ret.append(list(page.revisions(reverse=False, content=True)))\n",
    "    print(\"Revs\")\n",
    "\n",
    "def subGetMentions(revs, code, strings, ret):\n",
    "    ret.append(getMentions(revs, strings, code))\n",
    "    print(\"Mentions\")\n",
    "\n",
    "def subGetSizes(revs, code, ret):\n",
    "    ret.append(getSizes(revs, code))\n",
    "    print(\"Sizes\")\n",
    "\n",
    "def subGetEdits(revs, code, ret):\n",
    "    ret.append(getEdits(revs, code))\n",
    "    print(\"Edits\")\n",
    "\n",
    "def subGetViews(page, code, ret):\n",
    "    ret.append(getViews(page, code))\n",
    "    print(\"Views\")\n",
    "    \n",
    "def getSizes(revs, code):\n",
    "    df = pd.DataFrame([dict(r) for r in revs])\n",
    "    df = df[[\"userid\", \"timestamp\", \"size\"]]\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Get absolute size from relative size\n",
    "    df[\"diff\"] = (df['size'] - df['size'].shift(1)).abs()\n",
    "    df[\"diff\"] = df[\"diff\"].fillna(df[\"size\"])\n",
    "    \n",
    "    # Sample every month and shift by 1 day to get 1st of month\n",
    "    se = df[\"size\"].groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "    se.index = se.index.shift(1, freq=\"D\")\n",
    "    \n",
    "    return se.rename(code)\n",
    "\n",
    "def getEdits(revs, code):\n",
    "    df = pd.DataFrame([dict(r) for r in revs])\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    df[\"size\"] = 1\n",
    "\n",
    "    se = df[\"size\"].groupby(pd.Grouper(freq=\"1M\")).count()\n",
    "    se.index = se.index.shift(-1, freq=\"M\").shift(1, freq=\"D\")\n",
    "    \n",
    "    return se.rename(code)\n",
    "\n",
    "# Need to make it wiki-independant\n",
    "def getViews(page, code):\n",
    "    req = requests.get(pv_url % (page.title(), stime, etime), headers=pv_head)\n",
    "    se = pd.Series({datetime.strptime(str(item['timestamp'])[:-2], \"%Y%m%d\"): item['views'] for item in req.json()['items']}, name=\"Views\")\n",
    "    return se.rename(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcol = pd.read_pickle(os.path.join(\"pickles\", \"en_mentions.pkl\"))[\"Q50785019\"]\n",
    "testcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = testcol.last_valid_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetRevisions(page, ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetMentions(revs, \"blah\", epfl_alts, ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetSizes(revs, \"blah\", ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetEdits(revs, \"blah\", ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetViews(page, \"blah\", ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "t_pszs = threading.Thread(target=subGetSizes, args=(revs, \"blah\", ret,))\n",
    "t_pszs.start()\n",
    "t_pszs.join()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uts = updateTimeSeries(epfl_pages_sample, epfl_alts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(uts[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTimeSeries(pages, strings, rescan=False, flush=False):\n",
    "    '''\n",
    "    path = None if flush else os.path.join(\"pickles\", \"en_mentions.pkl\")\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_pickle(path)\n",
    "    except:\n",
    "        data = pd.DataFrame()\n",
    "        \n",
    "    dfs = [prev]\n",
    "    \n",
    "    if rescan:\n",
    "        pagecodes += list(prev.columns)\n",
    "\n",
    "    last_ts = data.index[-1]\n",
    "    '''\n",
    "    \n",
    "    # NEW\n",
    "    prev_code, prev_page = None, None\n",
    "    prev_revs = None\n",
    "    \n",
    "    dfs = [[] for i in range(4)]\n",
    "    \n",
    "    fail = []\n",
    "    \n",
    "    for curr_code, curr_page in tqdm(pages.items()):\n",
    "        # Not needed for now : prev[\"Q7526\"].last_valid_index()\n",
    "        # later we can see about getting only part of the revisions\n",
    "        # Techniquement la on discard le dernier, faut rajouter un dummy à la fin\n",
    "        \n",
    "        pmns, pszs, peds, pvws = [], [], [], []\n",
    "        curr_revs = []\n",
    "        \n",
    "        t_revs = threading.Thread(target=subGetRevisions, args=(curr_page, curr_revs,))\n",
    "        t_revs.start()\n",
    "\n",
    "        if prev_code:\n",
    "            t_pmns = threading.Thread(target=subGetMentions, args=(prev_revs, prev_code, strings, pmns,))\n",
    "            t_pszs = threading.Thread(target=subGetSizes, args=(prev_revs, prev_code, pszs,))\n",
    "            t_peds = threading.Thread(target=subGetEdits, args=(prev_revs, prev_code, peds,))\n",
    "            t_pvws = threading.Thread(target=subGetViews, args=(prev_page, prev_code, pvws,))\n",
    "\n",
    "            t_pmns.start()\n",
    "            t_pszs.start()\n",
    "            t_peds.start()\n",
    "            t_pvws.start()\n",
    "\n",
    "            t_pmns.join()\n",
    "            t_pszs.join()\n",
    "            t_peds.join()\n",
    "            t_pvws.join()\n",
    "\n",
    "        t_revs.join()\n",
    "        \n",
    "        prev_code = curr_code\n",
    "        prev_page = curr_page\n",
    "        prev_revs = curr_revs[0]\n",
    "\n",
    "        if prev_code:\n",
    "            for df_set, col in zip(dfs, [pmns, pszs, peds, pvws]):\n",
    "                try:\n",
    "                    df_set.append(col[0])\n",
    "                except:\n",
    "                    fail.append((prev_code, col))\n",
    "            \n",
    "    return dfs\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    END OF FUNCTION\n",
    "        \n",
    "        # NOW NEED TO PUT ALL ARRAYS TOGETHER\n",
    "\n",
    "        # Set limit timestamp (or None if no data yet)\n",
    "        ts = prev[code].last_valid_index() if code in prev.columns else None\n",
    "        \n",
    "        \n",
    "        if df is not None and len(df):\n",
    "            df.name = code\n",
    "            df = df.groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "            df.index = df.index.shift(1, freq=\"D\")\n",
    "            \n",
    "            # Combine with old data if it exists\n",
    "            if code in prev.columns:\n",
    "                df = df.combine_first(prev[code])\n",
    "\n",
    "            dfs.append(df)\n",
    "    \n",
    "    curr = pd.concat(dfs, axis=1)\n",
    "    curr = curr.ffill(axis=0)\n",
    "    curr.to_pickle(path)\n",
    "    \n",
    "    return curr\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages[:10]\n",
    "pcodes = [p.data_item().title() for p in epfl_pages[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_mentions = updateMentions(pcodes[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mtns = bulk_mentions.sum(axis=1)\n",
    "total_mtns.index.name = \"Date\"\n",
    "total_mtns.name = \"Total mentions\"\n",
    "total_mtns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = total_mtns.plot(legend=True, title=\"Mentions of EPFL on Wikipedia\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Mentions')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"mentions.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views of a page over time\n",
    "\n",
    "Here we compute the pageviews for each day and highlight the outliers.\n",
    "In the future we will compute outliers based on the local average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageviews(page, stime, etime):\n",
    "    req = requests.get(pv_url % (page.title(), stime, etime))\n",
    "    serie = pd.Series({datetime.strptime(str(item['timestamp'])[:-2], \"%Y%m%d\"): item['views'] for item in req.json()['items']}, name=\"Views\")\n",
    "    return serie\n",
    "\n",
    "def outliers(serie):\n",
    "    return pd.Series(serie[np.abs(stats.zscore(serie)) > 3], name=\"Outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs = pageviews(page, stime, etime)\n",
    "pvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = outliers(pvs)\n",
    "ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pvs.plot(legend=True, title=\"Daily views for 'Martin Vetterli' on Wikipedia\")\n",
    "ax = ols.plot(legend=True, ax=ax, linestyle=\"\", marker=\"o\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Views')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "\n",
    "for i, each in enumerate(ols.index):\n",
    "    y = ols[each]\n",
    "    ax.text(each + timedelta(25), y, y)\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"views.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, let's generate a list of all the pages that mention EPFL. We set `namespace=0` as this namespace is the one that contains regular pages. Note that searches from Pywikibot are ordered alphabetically by default, unlike what the Wikipedia API does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs2 = pageviews(page, stime, etime)\n",
    "to_rem = ols.index\n",
    "pvs2[to_rem] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be good to cover the whole dataset\n",
    "\n",
    "v = 100\n",
    "ax = pvs2.plot(legend=True)\n",
    "(pvs2.rolling(v, center=True).sum() / v).plot(ax=ax, legend=True, label=\"100-day average\", title=\"Daily views for 'Martin Vetterli' on Wikipedia, 100-day average without outliers\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Views')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"views__avg.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(page.revisions(reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageSize(page):\n",
    "    revs = list(page.revisions(reverse=True))\n",
    "    df = pd.DataFrame([dict(r) for r in revs])\n",
    "    df = df[[\"userid\", \"timestamp\", \"size\", \"minor\"]]\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Get absolute size from relative size\n",
    "    df[\"diff\"] = (df['size'] - df['size'].shift(1)).abs()\n",
    "    df[\"diff\"] = df[\"diff\"].fillna(df[\"size\"])\n",
    "    \n",
    "    # Sample every month and shift by 1 day to get 1st of month\n",
    "    df = df[\"size\"].groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "    df.index = df.index.shift(1, freq=\"D\")\n",
    "    \n",
    "    return df.rename(page.data_item().title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psz = getPageSize(page)\n",
    "psz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = psz.plot()\n",
    "ax.set_title(\"Size of Martin Vetterli's page\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Size in Bytes\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"mirko.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageScore(page):\n",
    "    # Get mentions\n",
    "    pms = getMentions(page, epfl_alts)\n",
    "    \n",
    "    if pms is None:\n",
    "        return None\n",
    "    \n",
    "    # Get page views\n",
    "    pvs = pageviews(page, stime, etime)\n",
    "    \n",
    "    # Get page size\n",
    "    psz = getPageSize(page)\n",
    "    \n",
    "    # Combine the data\n",
    "    df = pd.concat([pvs, psz.reindex(pvs.index), pms.reindex(pvs.index)], axis=1)\n",
    "    df = df.ffill(axis=0)\n",
    "    \n",
    "    s_prev = psz.index.difference(pvs.index)\n",
    "    s_fill = psz[s_prev[-1]] if len(s_prev) else 0\n",
    "    df[\"Size\"] = df[\"Size\"].fillna(s_fill)\n",
    "    \n",
    "    m_prev = pms.index.difference(pvs.index)\n",
    "    m_fill = pms[m_prev[-1]] if len(m_prev) else 0\n",
    "    df[\"Mentions\"] = df[\"Mentions\"].fillna(m_fill)\n",
    "    \n",
    "    # Generate score\n",
    "    df[\"Score\"] = df[\"Mentions\"] * df[\"Views\"] / df[\"Size\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psc = getPageScore(page)\n",
    "psc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editors\n",
    "\n",
    "We will consider a user that changed their name as a new user for simplicity's sake.  \n",
    "We do not use the function `page.contributors()` as it makes no distinction between regular and minor edits, and since we're going through the revisions we might as well extract that information in the process.\n",
    "\n",
    "This can be improved by weighing the edits depending on the size increase of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = list(page.revisions(reverse=True, content=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions to detect IP addresses and Bots.\n",
    "pat_ip = re.compile('^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$|([0-9a-fA-F][0-9a-fA-F]{0,3}:){7}([0-9a-fA-F][0-9a-fA-F]{0,3}){1}')\n",
    "pat_bot = re.compile(r'bot\\b', re.IGNORECASE)\n",
    "\n",
    "# Classify a user by its name into bots, real users, and IPs (unregistered users)\n",
    "def classify_user(name):\n",
    "    if pat_ip.match(name):\n",
    "        return \"IP\"\n",
    "    elif pat_bot.search(name):\n",
    "        return \"Bot\"\n",
    "    else:\n",
    "        return \"Real\"\n",
    "\n",
    "# Get user edit data from a page\n",
    "def users(revs):\n",
    "    # Get usernames and edit type\n",
    "    df = pd.DataFrame([(r[\"user\"], r[\"minor\"]) for r in revs])\n",
    "    \n",
    "    # Set as index and sort \n",
    "    df.index = pd.MultiIndex.from_frame(df)\n",
    "    df = df[[1]].sort_index(axis=0)\n",
    "    \n",
    "    # Group by user and edit type and split into columns\n",
    "    df = pd.DataFrame(df.groupby(level=[0,1]).size())\n",
    "    df = df.unstack(level=1, fill_value=0)\n",
    "    \n",
    "    # Rename columns and drop useless levels\n",
    "    df = df.droplevel(level=0, axis=1)\n",
    "    df.index.name = \"Usernames\"\n",
    "    df.columns = [\"Major\", \"Minor\"]\n",
    "    \n",
    "    # Add user types as first index level\n",
    "    df.index = pd.MultiIndex.from_tuples([classify_user(i), i] for i in df.index)\n",
    "    \n",
    "    return df.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ueds = users(revs)\n",
    "ueds[\"Edits\"] = ueds[\"Minor\"] + ueds[\"Major\"]\n",
    "\n",
    "ax = sns.swarmplot(x=ueds.index.get_level_values(0), y=\"Edits\", data=ueds)\n",
    "ax.set_title(\"Editors for 'Martin Vetterli' on Wikipedia\")\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"edits.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_users(pages):\n",
    "    # We'll want to get the bulk of the data out in the future\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    for p in pages:\n",
    "        curr = list(p.revisions(reverse=True, content=False))\n",
    "        udata = users(curr)\n",
    "        udata[p.title()] = udata[\"Minor\"] + udata[\"Major\"]\n",
    "        temp.append(udata[p.title()])\n",
    "    \n",
    "    return pd.concat(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udata = mass_users(epfl_pages[:100])\n",
    "udata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_sum = udata.sum(axis=1)\n",
    "usr_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_cnt = udata.count(axis=1)\n",
    "usr_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Scatterplot of user edits\")\n",
    "ax.set_xlabel(\"Number of edits\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "x_vals = np.linspace(0, 100)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (120, 95))\n",
    "fig.set_size_inches((9, 6))\n",
    "artists = []\n",
    "plot_names = []\n",
    "for (ns, s), (nc, c) in zip(usr_sum.groupby(level=0), usr_cnt.groupby(level=0)):\n",
    "    artists.append(plt.scatter(s, c))\n",
    "    plot_names.append(ns)\n",
    "ax.legend(artists, plot_names)\n",
    "ax.figure.savefig(os.path.join(im_path, \"users.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (ns, s), (nc, c) in zip(usr_sum.groupby(level=0), usr_cnt.groupby(level=0)):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extlink_data(page):\n",
    "    links = [link.split(\"/\", 5)[-1] if \"web.archive.org\" in link else link for link in page.extlinks()]\n",
    "    domains = pd.DataFrame([tldextract.extract(link) for link in links], columns=[\"subdomain\", \"domain\", \"suffix\"])\n",
    "    \n",
    "    try:\n",
    "        domains[\"tld\"] = domains[\"suffix\"].str.split(\".\", expand=False).apply(lambda e : e[-1])\n",
    "        domains[\"site\"] = domains[\"domain\"] + \".\" + domains[\"suffix\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sites = domains[\"site\"].value_counts()\n",
    "    tlds = domains[\"tld\"].value_counts()\n",
    "    return page.title(), sites, tlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, sites, tlds = extlink_data(page)\n",
    "tlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = epfl_pages[:500]\n",
    "\n",
    "series = np.array([extlink_data(t) for t in temp], dtype=object).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(s, name=t) for t, s in tuple(series[0:2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = pd.concat(series[1], axis=1, keys=series[0])\n",
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sum = refs.sum(axis=1)\n",
    "ref_sum[ref_sum > 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_cnt = refs.count(axis=1)\n",
    "ref_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(ref_sum, ref_cnt).axes\n",
    "ax.set_title(\"Scatterplot of domain repartition and frequency\")\n",
    "ax.set_xlabel(\"Number of occurences\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((9, 6))\n",
    "x_vals = np.linspace(0, 300)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (320, 280))\n",
    "for t in [\"epfl.ch\", \"unil.ch\", \"bbc.co.uk\", \"google.com\", \"nytimes.com\"]:\n",
    "    ax.scatter(ref_sum[t], ref_cnt[t], color=\"orange\")\n",
    "    ax.annotate(t, (ref_sum[t]+50, ref_cnt[t]+5), color=\"orange\", bbox=dict(facecolor='white', boxstyle=\"round,pad=0.3\"))\n",
    "ax.figure.savefig(os.path.join(im_path, \"domains.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_tld = pd.concat(series[2], axis=1, keys=series[0])\n",
    "refs_tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tld_sum = refs_tld.sum(axis=1)\n",
    "ref_tld_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tld_cnt = refs_tld.count(axis=1)\n",
    "ref_tld_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(ref_tld_sum, ref_tld_cnt).axes\n",
    "ax.set_title(\"Scatterplot of top-level-domain repartition and frequency\")\n",
    "ax.set_xlabel(\"Number of occurences\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((9, 6))\n",
    "x_vals = np.linspace(0, 450)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (720, 400))\n",
    "for t in [\"ch\", \"org\", \"com\", \"gov\"]:\n",
    "    ax.scatter(ref_tld_sum[t], ref_tld_cnt[t], color=\"orange\")\n",
    "    ax.annotate(t, (ref_tld_sum[t]+400, ref_tld_cnt[t]+1), color=\"orange\", bbox=dict(facecolor='white', boxstyle=\"round,pad=0.3\"))\n",
    "ax.figure.savefig(os.path.join(im_path, \"tlds.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make page to item and item to page functions\n",
    "# Heavily bottlenecked by the `revisions` function unfortunately\n",
    "\n",
    "# Gets given keys of all revisions after a given timestamp\n",
    "def getRevisionsTags(page, ts, keys, content=False):\n",
    "    gen = page.revisions(reverse=True, content=content)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            v = next(gen)\n",
    "            t = v[\"timestamp\"]\n",
    "            \n",
    "            if ts is None or t > ts:\n",
    "                yield t, [v[key] for key in keys]\n",
    "            else:\n",
    "                return\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "def updatePageSizes(pagecodes, rescan=False):\n",
    "    path = os.path.join(\"pickles\", \"en_page_sizes.pkl\")\n",
    "    \n",
    "    try:\n",
    "        prev = pd.read_pickle(path)\n",
    "    except:\n",
    "        prev = pd.DataFrame()\n",
    "        \n",
    "    dfs = [prev]\n",
    "    \n",
    "    if rescan:\n",
    "        pagecodes += list(prev.columns)\n",
    "    \n",
    "    for code in tqdm(pagecodes):\n",
    "        try:\n",
    "            # Recover page from code\n",
    "            p = pwb.Page(wiki_site, pwb.ItemPage(repo, code).sitelinks[\"enwiki\"].ns_title())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Set limit timestamp (or None if no data yet)\n",
    "        ts = prev[code].last_valid_index() if code in prev.columns else None\n",
    "        \n",
    "        # Get values after that timestamp\n",
    "        values = [(t, v[0]) for t, v in getRevisionsTags(p, ts, [\"size\"])]\n",
    "        \n",
    "        if len(values):\n",
    "            # Sample every month and shift by 1 day to get 1st of month\n",
    "            df = pd.DataFrame(values, columns=[0, code]).set_index(0)\n",
    "            df = df.groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "            df.index = df.index.shift(1, freq=\"D\")\n",
    "            \n",
    "            # Combine with old data if it exists\n",
    "            if code in prev.columns:\n",
    "                df = df.combine_first(prev[code])\n",
    "                \n",
    "\n",
    "            dfs.append(df)\n",
    "    \n",
    "    curr = pd.concat(dfs, axis=1)\n",
    "    curr = curr.ffill(axis=0)\n",
    "    curr.to_pickle(path)\n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcodes = [p.data_item().title() for p in epfl_pages[:300]]\n",
    "pszs = updatePageSizes(pcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pszs = pd.read_pickle(\"pickles/en_page_sizes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pszs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = pszs.sum(axis=1)\n",
    "total_size.index.name = \"Date\"\n",
    "total_size.name = \"Select Pages Size\"\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page count\n",
    "\n",
    "Here we look at how many of our pages exist at a given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageCounts(df):\n",
    "    ret = df.count(axis=1)\n",
    "    ret.index.name = \"Date\"\n",
    "    ret.name = \"Select Pages Count\"\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnts = pageCounts(pszs)\n",
    "pcnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page size and page counts ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(total_size / pcnts).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWikistats(fn, name, exp=0):\n",
    "    path = os.path.join(\"csv\", \"wikistats\", fn)\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    df.index = pd.to_datetime(df[\"month\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").rename(\"Date\")\n",
    "    df = df[\"total.total\"].rename(name) * 10**exp\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_size = loadWikistats(\"size_change_en.csv\", name=\"English Wikipedia Size\", exp=0)\n",
    "wiki_size = wiki_size[17:].cumsum()\n",
    "wiki_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_cnts = loadWikistats(\"pages_en.csv\", name=\"English Wikipedia Count\", exp=0)\n",
    "wiki_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_ratio = pd.concat([wiki_size, wiki_cnts], axis=1).dropna()\n",
    "wiki_ratio = wiki_ratio.iloc[:,0] / wiki_ratio.iloc[:,1]\n",
    "wiki_ratio.name = \"English Wikipedia Ratio\"\n",
    "wiki_ratio.plot()\n",
    "wiki_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_ratio = pd.concat([total_size, pcnts], axis=1).dropna()\n",
    "pages_ratio = pages_ratio.iloc[:,0] / pages_ratio.iloc[:,1]\n",
    "pages_ratio.name = \"Select Pages Ratio\"\n",
    "pages_ratio.plot()\n",
    "pages_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([total_size], axis=1).plot(logy=False, figsize=(9, 6))\n",
    "ax.set_title(\"Size in Bytes\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"size_comp_lin.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([total_size, wiki_size], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Size in Bytes (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"size_comp.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([pcnts, wiki_cnts], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Page counts (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"count_comp.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([pages_ratio, wiki_ratio], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Size / Page Count Ratio (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"ratio_comp.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to make a record of what pages have indeed mentions or not so we can refer to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(page):\n",
    "    ret = getPageScore(page)\n",
    "    \n",
    "    if ret is None:\n",
    "        return None, None\n",
    "    \n",
    "    idx = page.data_item().title()\n",
    "    ret.to_pickle(os.path.join(\"pickles\", idx))\n",
    "    return idx, ret\n",
    "\n",
    "def loadData(idx):\n",
    "    return pd.read_pickle(os.path.join(\"pickles\", idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[en:Whistleblower]]\n",
    "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entry over time\n",
    "* list of incoming links\n",
    "* list of outgoing links\n",
    "* mention history\n",
    "\n",
    "work with user pages  \n",
    "potentially use the wikipedia package for smaller operations as it seems to be faster, maybe do some timed tests\n",
    "\n",
    "count improve counts by checking revisions 5 before and 5 after to ignore edit wars and such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to handle the problem or redirects and page name changes in the future.\n",
    "\n",
    "Will need to classify whether we found the subject through keywords or if EPFL was mentioned.\n",
    "\n",
    "We assume mentions are only added for performance and simplicity reasons. It's very rare that content will be removed from pages.\n",
    "\n",
    "Link pages with contributors\n",
    "\n",
    "Update data over time instead of recomputing everything\n",
    "\n",
    "Bot that changes names of EPFL or makes a suggestion on the talk page\n",
    "\n",
    "orcid, unique identifier for papers\n",
    "\n",
    "https://go.epfl.ch/wikiproject\n",
    "\n",
    "correlate growth of epfl mentions and wikipedia in general (or science related pages, lets see what we can do)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki-env",
   "language": "python",
   "name": "wiki-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
