{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, requests, threading, time, tldextract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import wikipedia as wp\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing our custom functions\n",
    "from wiki_workers import *\n",
    "\n",
    "# Configuring matplotlib to output opaque images to avoid issues with dark mode\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"savefig.facecolor\"] = \"w\"\n",
    "\n",
    "# We do only read operations, therefore no user config is necessary.\n",
    "# Normally the system crashes when there is no user config unless we tell it otherwise with this environment variable.\n",
    "#   0 is default\n",
    "#   1 means ignore the config\n",
    "#   2 means ignore the config and don't throw warnings\n",
    "os.environ[\"PYWIKIBOT_NO_USER_CONFIG\"] = \"2\"\n",
    "\n",
    "# Now we can import pywikibot\n",
    "import pywikibot as pwb\n",
    "import pywikibot.data.api as api\n",
    "\n",
    "# Then we can setup references for Wikipedia and Wikidata\n",
    "wiki_site = pwb.Site(code=\"en\", fam=\"wikipedia\")\n",
    "data_site = pwb.Site(code=\"wikidata\", fam=\"wikidata\")\n",
    "repo = data_site.data_repository()\n",
    "\n",
    "# Setting paths\n",
    "im_path = \"graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We list here the search terms for EPFL\n",
    "epfl_alts = [\n",
    "    \"EPFL\",\n",
    "    \"École Polytechnique Fédérale de Lausanne\",\n",
    "    \"Swiss Federal Institute of Technology in Lausanne\",\n",
    "    \"EPF Lausanne\",\n",
    "    \"ETH Lausanne\",\n",
    "    \"Poly Lausanne\",\n",
    "]\n",
    "\n",
    "# And here are some other universities so we can compare data\n",
    "unil_alts = [\n",
    "    \"Unil\",\n",
    "    \"Université de Lausanne\",\n",
    "    \"Uni Lausanne\",\n",
    "    \"University of Lausanne\",\n",
    "    \"Lausanne University\",\n",
    "]\n",
    "\n",
    "ethz_alts = [\n",
    "    \"ETHZ\",\n",
    "    \"EPFZ\",\n",
    "    \"Eidgenössische Technische Hochschule Zürich\",\n",
    "    \"ETH Zurich\",\n",
    "    \"Swiss Federal Institute of Technology in Zurich\",\n",
    "    \"École Polytechnique Fédérale de Lausanne\",\n",
    "    \"Poly Zurich\"\n",
    "]\n",
    "\n",
    "mit_alts = [\n",
    "    \"MIT\",\n",
    "    \"Massachusetts Institute of Technology\",\n",
    "    \"Boston Tech\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a list of all the pages that mention EPFL. We set `namespace=0` as this namespace is the one that contains regular pages. Note that searches from Pywikibot are ordered alphabetically by default, unlike what the Wikipedia API does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages = collect_pages(wiki_site, epfl_alts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that Wikipedia started storing page view statistics since July 1st, 2015. This means we will not have any data available before that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePages(\"EPFL\", epfl_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages = loadPages(\"EPFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in list(tableau[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "[sys.getsizeof(x) for x in tableau[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []\n",
    "pulled_data = []\n",
    "left = list(epfl_pages.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Processed 11\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sleeping for 5.0 seconds, 2021-03-06 20:56:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 46\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 54\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 66\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 104\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 122\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n",
      "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 130\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sleeping for 5.0 seconds, 2021-03-06 22:58:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 132\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sleeping for 5.0 seconds, 2021-03-06 23:01:58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interrupted!3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\"Start\")\n",
    "\n",
    "try:\n",
    "    while left:\n",
    "        key, item = left.pop()\n",
    "        pulled_data.append(item.revisions(content=True))\n",
    "        done.append(key)\n",
    "        i += 1\n",
    "        print(\"Processed\", i, end=\"\\r\")\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 3/5083 [01:33<44:00:30, 31.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-add4c7afbf79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtableau\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepfl_pages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtableau\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrevisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\tools\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*__args, **__kw)\u001b[0m\n\u001b[0;32m   1447\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0m__kw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_arg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\page\\__init__.py\u001b[0m in \u001b[0;36mrevisions\u001b[1;34m(self, reverse, total, content, starttime, endtime)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[1;34m\"\"\"Generator which loads the version history as Revision instances.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m         \u001b[1;31m# TODO: Only request uncached revisions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         self.site.loadrevisions(self, content=content, rvdir=reverse,\n\u001b[0m\u001b[0;32m   1693\u001b[0m                                 \u001b[0mstarttime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstarttime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m                                 total=total)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\tools\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*__args, **__kw)\u001b[0m\n\u001b[0;32m   1447\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0m__kw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_arg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\tools\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*__args, **__kw)\u001b[0m\n\u001b[0;32m   1530\u001b[0m                              \"', '\".join(deprecated)),\n\u001b[0;32m   1531\u001b[0m                      DeprecationWarning, depth)\n\u001b[1;32m-> 1532\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0mmanage_wrapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\site\\__init__.py\u001b[0m in \u001b[0;36mloadrevisions\u001b[1;34m(self, page, content, section, **kwargs)\u001b[0m\n\u001b[0;32m   3417\u001b[0m             \u001b[0mrvgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_maximum_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# suppress use of rvlimit parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3419\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mpagedata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrvgen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3420\u001b[0m             if not self.sametitle(pagedata['title'],\n\u001b[0;32m   3421\u001b[0m                                   page.title(with_section=False)):\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\data\\api.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2850\u001b[0m         \u001b[1;34m\"\"\"Yield results.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_previous_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2852\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2853\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_previous_dicts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\data\\api.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2694\u001b[0m                 prev_limit, new_limit, previous_result_had_data)\n\u001b[0;32m   2695\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2696\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2697\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2698\u001b[0m                 pywikibot.debug(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\data\\api.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1872\u001b[0m             use_get, uri, body, headers = self._get_request_params(use_get,\n\u001b[0;32m   1873\u001b[0m                                                                    paramstring)\n\u001b[1;32m-> 1874\u001b[1;33m             rawdata, use_get = self._http_request(use_get, uri, body, headers,\n\u001b[0m\u001b[0;32m   1875\u001b[0m                                                   paramstring)\n\u001b[0;32m   1876\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrawdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\data\\api.py\u001b[0m in \u001b[0;36m_http_request\u001b[1;34m(self, use_get, uri, body, headers, paramstring)\u001b[0m\n\u001b[0;32m   1608\u001b[0m         \"\"\"\n\u001b[0;32m   1609\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m             data = http.request(\n\u001b[0m\u001b[0;32m   1611\u001b[0m                 \u001b[0msite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muri\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'GET'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_get\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\tools\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*__args, **__kw)\u001b[0m\n\u001b[0;32m   1447\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0m__kw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_arg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\comms\\http.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(site, uri, method, params, body, headers, data, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0mbaseuri\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseuri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m     \u001b[0msite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrottle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretry_after\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse_headers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'retry-after'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\tools\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*__args, **__kw)\u001b[0m\n\u001b[0;32m   1447\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0m__kw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_arg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\pywikibot\\comms\\http.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(uri, method, params, body, headers, default_error_handling, use_fake_user_agent, data, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;31m# HTTPS request can succeed even if the certificate is invalid and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;31m# verify=True, when a request with verify=False happened before\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m         response = session.request(method, uri, params=params, data=body,\n\u001b[0m\u001b[0;32m    515\u001b[0m                                    \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                                    \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mignore_validation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wiki-env\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tableau = []\n",
    "for c, p in tqdm(epfl_pages.items()):\n",
    "    tableau.append(p.revisions(content=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages_sample = dict([(key, epfl_pages[key]) for key in epfl_pages][40:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages_sample = collect_pages(wiki_site, epfl_alts, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from wiki_workers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in batches(list(epfl_pages_sample.items()), 2):\n",
    "    x = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uts = updateTimeSeries(epfl_pages_sample, epfl_alts, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(uts))\n",
    "uts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(epfl_pages_sample.items())[1][1].getOldVersion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, page in epfl_pages_sample.items():\n",
    "    r = list(page.revisions(reverse=False, content=False))\n",
    "\n",
    "    name = \"test\"\n",
    "    fn = name + \".pkl\"\n",
    "    path = fn\n",
    "\n",
    "    with open(path, \"wb\") as f:\n",
    "        pkl.dump(r, f)\n",
    "\n",
    "    print(os.path.getsize(path))\n",
    "    print(page.revision_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(epfl_pages_sample.items())[1][1].getOldVersion(1006458075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(list(epfl_pages_sample.items())[1][1].revisions(content=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temppage = list(epfl_pages_sample.items())[2][1]\n",
    "temppage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temprevs = list(temppage.revisions(content=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempseries = pd.Series({r[\"timestamp\"] : r[\"revid\"] for r in temprevs}).groupby(pd.Grouper(freq=\"1M\")).nth(-1)\n",
    "tempseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to get extra data but not for initial sweep\n",
    "start = time.time()\n",
    "[temppage.getOldVersion(x) for x in tempseries]\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "temppage.revisions(content=True)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing a page\n",
    "\n",
    "Let's look at the different ways we can refer to a given page. We will be using Martin Vetterli's page for our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get a page by name\n",
    "page = pwb.Page(wiki_site, u\"Martin Vetterli\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get its data reference\n",
    "item = page.data_item()\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the reference directly\n",
    "item = pwb.ItemPage(repo, \"Q6776811\")\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can get all the pages linked to this reference through WikiData\n",
    "for k, v in dict(item.sitelinks).items():\n",
    "    print(k + \"\\n\\t\" + v.ns_title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = api.Request(site=data_site, parameters={'action': 'query',\n",
    "                                                'titles': item,\n",
    "                                                'prop': 'pageviews',\n",
    "                                                'pvipdays': 1000})\n",
    "\n",
    "print(\"As the warning says, the default API request gives us a maximum of\", len(req.submit()['query']['pages'][str(item.pageid)]['pageviews']), \"days of data. This is insufficient.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dates\n",
    "pp_first = datetime.strptime(stime, \"%Y%m%d00\").strftime(\"%B %d, %Y\").replace(\" 0\", \" \")\n",
    "pp_today = datetime.today().strftime(\"%B %d, %Y\").replace(\" 0\", \" \")\n",
    "\n",
    "# Request data\n",
    "page_name = \"Martin_Vetterli\"\n",
    "r = requests.get(pv_url % (page_name, stime, etime), headers=pv_head)\n",
    "print(\"Digging deeper into the API, we see we have access to\", len(r.json()['items']), \"days of pageview data since\", pp_first, \"as of\", pp_today + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentions of EPFL in a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagecounts(page, strings):\n",
    "    return [page.text.count(s) for s in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pagecounts(page, epfl_alts)\n",
    "dict(zip(epfl_alts, pcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentions over time\n",
    "\n",
    "For `getPageChanges`, we recover all revisions at once at it is significantly faster to do that than to call `page.getOldVersion` continuously.\n",
    "\n",
    "It is important to note that our process is simplified to improve performance by minimizing the number of revisions requested; if a page had 5 mentions in 2008, 5 in 2010, but 4 in 2009, the 4 will be glossed over as we assume the amount of such cases will be rare and insignificant.\n",
    "\n",
    "Getting the revisions without the content is approximately 7 times faster on large pages (35 vs. 5 seconds), while getting the text from the revisions or from `getOldVersion` takes the same amount of time. However we are still getting faster results for revisions with content when it is part of our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.revision_count()\n",
    "\n",
    "start = time.time()\n",
    "list(page.revisions())\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "[x for x in page.revisions()]\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMentionCounts(rev, strings):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(revs, strings, idx):\n",
    "    text = revs[idx]\n",
    "    return sum([text.text.count(s) for s in strings])\n",
    "\n",
    "def getOrUpdate(revs, strings, counts, idx, changes):\n",
    "    if idx not in counts:\n",
    "        temp = getCounts(revs, strings, idx)\n",
    "        counts[idx] = temp\n",
    "        \n",
    "        # Do not consider the count if an earlier revision had more\n",
    "        if not any([counts[k] > temp for k in counts.keys() if k < idx]):\n",
    "            changes[temp] = min(changes.get(temp) or idx, idx) \n",
    "    \n",
    "    return counts[idx]\n",
    "\n",
    "def getMentions(revs_flip, strings, code):\n",
    "    if not getCounts(revs_flip, strings, 0):\n",
    "        return None\n",
    "    \n",
    "    # Reversing revisions\n",
    "    revs = revs_flip[::-1]\n",
    "\n",
    "    # Start with whole scope\n",
    "    queue = [(0, len(revs) - 1)]\n",
    "    \n",
    "    # To avoid double checking revisions we store the counts here\n",
    "    cnts = {}\n",
    "    \n",
    "    # And here we store the count-index pairs\n",
    "    changes = {}\n",
    "\n",
    "    while queue:\n",
    "        # Process first element\n",
    "        r0, r1 = queue[0]\n",
    "        queue = queue[1:]\n",
    "\n",
    "        # Only proceed if current scope covers multiple indices\n",
    "        if r0 != r1:\n",
    "            # Get counts for both indices\n",
    "            v0 = getOrUpdate(revs, strings, cnts, r0, changes)\n",
    "            v1 = getOrUpdate(revs, strings, cnts, r1, changes)\n",
    "\n",
    "            # Only proceed if there is a change of count in the current scope\n",
    "            if v0 != v1 and abs(r1 - r0) > 1:\n",
    "                mid = (r0 + r1) // 2\n",
    "                queue.extend([(r0, mid), (mid, r1)])\n",
    "\n",
    "    changes = {revs[v][\"timestamp\"]: k for k, v in changes.items()}\n",
    "    changes = {datetime.combine(k.date(), k.time()): v for k, v in changes.items()}\n",
    "    \n",
    "    # Here we simplify our data to a maximum of one point per month (we take the last one)\n",
    "    changes = pd.Series(changes, name=\"Mentions\").sort_index().groupby(pd.Grouper(freq=\"1M\")).nth(-1)\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pms = getMentions(page, epfl_alts)\n",
    "pms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pms.plot(linestyle='--', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORARY, NEED TO ABSTRACT THE ARRAY CREATION PROCESS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"pickles\", \"en_mentions.pkl\")\n",
    "prev = pd.read_pickle(path)\n",
    "prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1:2,3:4}\n",
    "\n",
    "for a, b in d.items():\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = list(page.revisions(reverse=False, content=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subGetRevisions(page, ret, minDate=None):\n",
    "    # Need to implement minDate functionality\n",
    "    ret.append(list(page.revisions(reverse=False, content=True)))\n",
    "    print(\"Revs\")\n",
    "\n",
    "def subGetMentions(revs, code, strings, ret):\n",
    "    ret.append(getMentions(revs, strings, code))\n",
    "    print(\"Mentions\")\n",
    "\n",
    "def subGetSizes(revs, code, ret):\n",
    "    ret.append(getSizes(revs, code))\n",
    "    print(\"Sizes\")\n",
    "\n",
    "def subGetEdits(revs, code, ret):\n",
    "    ret.append(getEdits(revs, code))\n",
    "    print(\"Edits\")\n",
    "\n",
    "def subGetViews(page, code, ret):\n",
    "    ret.append(getViews(page, code))\n",
    "    print(\"Views\")\n",
    "    \n",
    "def getSizes(revs, code):\n",
    "    df = pd.DataFrame([dict(r) for r in revs])\n",
    "    df = df[[\"userid\", \"timestamp\", \"size\"]]\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Get absolute size from relative size\n",
    "    df[\"diff\"] = (df['size'] - df['size'].shift(1)).abs()\n",
    "    df[\"diff\"] = df[\"diff\"].fillna(df[\"size\"])\n",
    "    \n",
    "    # Sample every month and shift by 1 day to get 1st of month\n",
    "    se = df[\"size\"].groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "    se.index = se.index.shift(1, freq=\"D\")\n",
    "    \n",
    "    return se.rename(code)\n",
    "\n",
    "def getEdits(revs, code):\n",
    "    df = pd.DataFrame([dict(r) for r in revs])\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    df[\"size\"] = 1\n",
    "\n",
    "    se = df[\"size\"].groupby(pd.Grouper(freq=\"1M\")).count()\n",
    "    se.index = se.index.shift(-1, freq=\"M\").shift(1, freq=\"D\")\n",
    "    \n",
    "    return se.rename(code)\n",
    "\n",
    "# Need to make it wiki-independant\n",
    "def getViews(page, code):\n",
    "    req = requests.get(pv_url % (page.title(), stime, etime), headers=pv_head)\n",
    "    se = pd.Series({datetime.strptime(str(item['timestamp'])[:-2], \"%Y%m%d\"): item['views'] for item in req.json()['items']}, name=\"Views\")\n",
    "    return se.rename(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcol = pd.read_pickle(os.path.join(\"pickles\", \"en_mentions.pkl\"))[\"Q50785019\"]\n",
    "testcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = testcol.last_valid_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetRevisions(page, ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetMentions(revs, \"blah\", epfl_alts, ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetSizes(revs, \"blah\", ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetEdits(revs, \"blah\", ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "subGetViews(page, \"blah\", ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "t_pszs = threading.Thread(target=subGetSizes, args=(revs, \"blah\", ret,))\n",
    "t_pszs.start()\n",
    "t_pszs.join()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2 = ret[0][0][\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ts1)\n",
    "print(ts2)\n",
    "print(ts1 - ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uts = updateTimeSeries(epfl_pages_sample, epfl_alts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(uts[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTimeSeries(pages, strings, rescan=False, flush=False):\n",
    "    '''\n",
    "    path = None if flush else os.path.join(\"pickles\", \"en_mentions.pkl\")\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_pickle(path)\n",
    "    except:\n",
    "        data = pd.DataFrame()\n",
    "        \n",
    "    dfs = [prev]\n",
    "    \n",
    "    if rescan:\n",
    "        pagecodes += list(prev.columns)\n",
    "\n",
    "    last_ts = data.index[-1]\n",
    "    '''\n",
    "    \n",
    "    # NEW\n",
    "    prev_code, prev_page = None, None\n",
    "    prev_revs = None\n",
    "    \n",
    "    dfs = [[] for i in range(4)]\n",
    "    \n",
    "    fail = []\n",
    "    \n",
    "    for curr_code, curr_page in tqdm(pages.items()):\n",
    "        # Not needed for now : prev[\"Q7526\"].last_valid_index()\n",
    "        # later we can see about getting only part of the revisions\n",
    "        # Techniquement la on discard le dernier, faut rajouter un dummy à la fin\n",
    "        \n",
    "        pmns, pszs, peds, pvws = [], [], [], []\n",
    "        curr_revs = []\n",
    "        \n",
    "        t_revs = threading.Thread(target=subGetRevisions, args=(curr_page, curr_revs,))\n",
    "        t_revs.start()\n",
    "\n",
    "        if prev_code:\n",
    "            t_pmns = threading.Thread(target=subGetMentions, args=(prev_revs, prev_code, strings, pmns,))\n",
    "            t_pszs = threading.Thread(target=subGetSizes, args=(prev_revs, prev_code, pszs,))\n",
    "            t_peds = threading.Thread(target=subGetEdits, args=(prev_revs, prev_code, peds,))\n",
    "            t_pvws = threading.Thread(target=subGetViews, args=(prev_page, prev_code, pvws,))\n",
    "\n",
    "            t_pmns.start()\n",
    "            t_pszs.start()\n",
    "            t_peds.start()\n",
    "            t_pvws.start()\n",
    "\n",
    "            t_pmns.join()\n",
    "            t_pszs.join()\n",
    "            t_peds.join()\n",
    "            t_pvws.join()\n",
    "\n",
    "        t_revs.join()\n",
    "        \n",
    "        prev_code = curr_code\n",
    "        prev_page = curr_page\n",
    "        prev_revs = curr_revs[0]\n",
    "\n",
    "        if prev_code:\n",
    "            for df_set, col in zip(dfs, [pmns, pszs, peds, pvws]):\n",
    "                try:\n",
    "                    df_set.append(col[0])\n",
    "                except:\n",
    "                    fail.append((prev_code, col))\n",
    "            \n",
    "    return dfs\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    END OF FUNCTION\n",
    "        \n",
    "        # NOW NEED TO PUT ALL ARRAYS TOGETHER\n",
    "\n",
    "        # Set limit timestamp (or None if no data yet)\n",
    "        ts = prev[code].last_valid_index() if code in prev.columns else None\n",
    "        \n",
    "        \n",
    "        if df is not None and len(df):\n",
    "            df.name = code\n",
    "            df = df.groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "            df.index = df.index.shift(1, freq=\"D\")\n",
    "            \n",
    "            # Combine with old data if it exists\n",
    "            if code in prev.columns:\n",
    "                df = df.combine_first(prev[code])\n",
    "\n",
    "            dfs.append(df)\n",
    "    \n",
    "    curr = pd.concat(dfs, axis=1)\n",
    "    curr = curr.ffill(axis=0)\n",
    "    curr.to_pickle(path)\n",
    "    \n",
    "    return curr\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages[:10]\n",
    "pcodes = [p.data_item().title() for p in epfl_pages[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_mentions = updateMentions(pcodes[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mtns = bulk_mentions.sum(axis=1)\n",
    "total_mtns.index.name = \"Date\"\n",
    "total_mtns.name = \"Total mentions\"\n",
    "total_mtns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = total_mtns.plot(legend=True, title=\"Mentions of EPFL on Wikipedia\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Mentions')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"mentions.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED TO MAKE THE PCODES FASTER\n",
    "\n",
    "Par exemple en mémorisant les codes des pages  \n",
    "Stocker les bulk en sparse matrix avec des valeurs uniquement la ou ca change et pour generer le graphe on fait juste un pulldown  \n",
    "Sauver régulièrement, mettre un intervalle par défaut  \n",
    "PARALLELIZE  \n",
    "Si la reference existe deja, aller avec des previous revisions au lieu d'appeler revisions (faire un test pour voir la différence de performance), on assume que les collectes de données se font régulièrement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views of a page over time\n",
    "\n",
    "Here we compute the pageviews for each day and highlight the outliers.\n",
    "In the future we will compute outliers based on the local average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageviews(page, stime, etime):\n",
    "    req = requests.get(pv_url % (page.title(), stime, etime))\n",
    "    serie = pd.Series({datetime.strptime(str(item['timestamp'])[:-2], \"%Y%m%d\"): item['views'] for item in req.json()['items']}, name=\"Views\")\n",
    "    return serie\n",
    "\n",
    "def outliers(serie):\n",
    "    return pd.Series(serie[np.abs(stats.zscore(serie)) > 3], name=\"Outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs = pageviews(page, stime, etime)\n",
    "pvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = outliers(pvs)\n",
    "ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pvs.plot(legend=True, title=\"Daily views for 'Martin Vetterli' on Wikipedia\")\n",
    "ax = ols.plot(legend=True, ax=ax, linestyle=\"\", marker=\"o\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Views')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "\n",
    "for i, each in enumerate(ols.index):\n",
    "    y = ols[each]\n",
    "    ax.text(each + timedelta(25), y, y)\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"views.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, let's generate a list of all the pages that mention EPFL. We set `namespace=0` as this namespace is the one that contains regular pages. Note that searches from Pywikibot are ordered alphabetically by default, unlike what the Wikipedia API does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs2 = pageviews(page, stime, etime)\n",
    "to_rem = ols.index\n",
    "pvs2[to_rem] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be good to cover the whole dataset\n",
    "\n",
    "v = 100\n",
    "ax = pvs2.plot(legend=True)\n",
    "(pvs2.rolling(v, center=True).sum() / v).plot(ax=ax, legend=True, label=\"100-day average\", title=\"Daily views for 'Martin Vetterli' on Wikipedia, 100-day average without outliers\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Views')\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"views__avg.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlinks\n",
    "\n",
    "So far I haven't found an efficient way to account for backlinks in page revisions. Therefore this will be skipped for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(page.revisions(reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageSize(page):\n",
    "    revs = list(page.revisions(reverse=True))\n",
    "    df = pd.DataFrame([dict(r) for r in revs])\n",
    "    df = df[[\"userid\", \"timestamp\", \"size\", \"minor\"]]\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    \n",
    "    # Get absolute size from relative size\n",
    "    df[\"diff\"] = (df['size'] - df['size'].shift(1)).abs()\n",
    "    df[\"diff\"] = df[\"diff\"].fillna(df[\"size\"])\n",
    "    \n",
    "    # Sample every month and shift by 1 day to get 1st of month\n",
    "    df = df[\"size\"].groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "    df.index = df.index.shift(1, freq=\"D\")\n",
    "    \n",
    "    return df.rename(page.data_item().title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psz = getPageSize(page)\n",
    "psz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = psz.plot()\n",
    "ax.set_title(\"Size of Martin Vetterli's page\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Size in Bytes\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"mirko.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageScore(page):\n",
    "    # Get mentions\n",
    "    pms = getMentions(page, epfl_alts)\n",
    "    \n",
    "    if pms is None:\n",
    "        return None\n",
    "    \n",
    "    # Get page views\n",
    "    pvs = pageviews(page, stime, etime)\n",
    "    \n",
    "    # Get page size\n",
    "    psz = getPageSize(page)\n",
    "    \n",
    "    # Combine the data\n",
    "    df = pd.concat([pvs, psz.reindex(pvs.index), pms.reindex(pvs.index)], axis=1)\n",
    "    df = df.ffill(axis=0)\n",
    "    \n",
    "    s_prev = psz.index.difference(pvs.index)\n",
    "    s_fill = psz[s_prev[-1]] if len(s_prev) else 0\n",
    "    df[\"Size\"] = df[\"Size\"].fillna(s_fill)\n",
    "    \n",
    "    m_prev = pms.index.difference(pvs.index)\n",
    "    m_fill = pms[m_prev[-1]] if len(m_prev) else 0\n",
    "    df[\"Mentions\"] = df[\"Mentions\"].fillna(m_fill)\n",
    "    \n",
    "    # Generate score\n",
    "    df[\"Score\"] = df[\"Mentions\"] * df[\"Views\"] / df[\"Size\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psc = getPageScore(page)\n",
    "psc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editors\n",
    "\n",
    "We will consider a user that changed their name as a new user for simplicity's sake.  \n",
    "We do not use the function `page.contributors()` as it makes no distinction between regular and minor edits, and since we're going through the revisions we might as well extract that information in the process.\n",
    "\n",
    "This can be improved by weighing the edits depending on the size increase of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = list(page.revisions(reverse=True, content=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions to detect IP addresses and Bots.\n",
    "pat_ip = re.compile('^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$|([0-9a-fA-F][0-9a-fA-F]{0,3}:){7}([0-9a-fA-F][0-9a-fA-F]{0,3}){1}')\n",
    "pat_bot = re.compile(r'bot\\b', re.IGNORECASE)\n",
    "\n",
    "# Classify a user by its name into bots, real users, and IPs (unregistered users)\n",
    "def classify_user(name):\n",
    "    if pat_ip.match(name):\n",
    "        return \"IP\"\n",
    "    elif pat_bot.search(name):\n",
    "        return \"Bot\"\n",
    "    else:\n",
    "        return \"Real\"\n",
    "\n",
    "# Get user edit data from a page\n",
    "def users(revs):\n",
    "    # Get usernames and edit type\n",
    "    df = pd.DataFrame([(r[\"user\"], r[\"minor\"]) for r in revs])\n",
    "    \n",
    "    # Set as index and sort \n",
    "    df.index = pd.MultiIndex.from_frame(df)\n",
    "    df = df[[1]].sort_index(axis=0)\n",
    "    \n",
    "    # Group by user and edit type and split into columns\n",
    "    df = pd.DataFrame(df.groupby(level=[0,1]).size())\n",
    "    df = df.unstack(level=1, fill_value=0)\n",
    "    \n",
    "    # Rename columns and drop useless levels\n",
    "    df = df.droplevel(level=0, axis=1)\n",
    "    df.index.name = \"Usernames\"\n",
    "    df.columns = [\"Major\", \"Minor\"]\n",
    "    \n",
    "    # Add user types as first index level\n",
    "    df.index = pd.MultiIndex.from_tuples([classify_user(i), i] for i in df.index)\n",
    "    \n",
    "    return df.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ueds = users(revs)\n",
    "ueds[\"Edits\"] = ueds[\"Minor\"] + ueds[\"Major\"]\n",
    "\n",
    "ax = sns.swarmplot(x=ueds.index.get_level_values(0), y=\"Edits\", data=ueds)\n",
    "ax.set_title(\"Editors for 'Martin Vetterli' on Wikipedia\")\n",
    "ax.figure.set_size_inches((9, 6))\n",
    "    \n",
    "ax.figure.savefig(os.path.join(im_path, \"edits.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_users(pages):\n",
    "    # We'll want to get the bulk of the data out in the future\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    for p in pages:\n",
    "        curr = list(p.revisions(reverse=True, content=False))\n",
    "        udata = users(curr)\n",
    "        udata[p.title()] = udata[\"Minor\"] + udata[\"Major\"]\n",
    "        temp.append(udata[p.title()])\n",
    "    \n",
    "    return pd.concat(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udata = mass_users(epfl_pages[:100])\n",
    "udata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_sum = udata.sum(axis=1)\n",
    "usr_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_cnt = udata.count(axis=1)\n",
    "usr_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Scatterplot of user edits\")\n",
    "ax.set_xlabel(\"Number of edits\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "x_vals = np.linspace(0, 100)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (120, 95))\n",
    "fig.set_size_inches((9, 6))\n",
    "artists = []\n",
    "plot_names = []\n",
    "for (ns, s), (nc, c) in zip(usr_sum.groupby(level=0), usr_cnt.groupby(level=0)):\n",
    "    artists.append(plt.scatter(s, c))\n",
    "    plot_names.append(ns)\n",
    "ax.legend(artists, plot_names)\n",
    "ax.figure.savefig(os.path.join(im_path, \"users.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (ns, s), (nc, c) in zip(usr_sum.groupby(level=0), usr_cnt.groupby(level=0)):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to make a record of what pages have indeed mentions or not so we can refer to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(page):\n",
    "    ret = getPageScore(page)\n",
    "    \n",
    "    if ret is None:\n",
    "        return None, None\n",
    "    \n",
    "    idx = page.data_item().title()\n",
    "    ret.to_pickle(os.path.join(\"pickles\", idx))\n",
    "    return idx, ret\n",
    "\n",
    "def loadData(idx):\n",
    "    return pd.read_pickle(os.path.join(\"pickles\", idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "fns = os.listdir(\"pickles\")\n",
    "for p in epfl_pages:\n",
    "    try:\n",
    "        if p.data_item().title() not in fns:\n",
    "            saveData(p)\n",
    "            print(p)\n",
    "    except:\n",
    "        pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[en:Whistleblower]]\n",
    "WARNING: API warning (result): This result was truncated because it would otherwise be larger than the limit of 12,582,912 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banane = pwb.Page(wiki_site, u\"Fréquence Banane\")\n",
    "banane_s = getPageScore(banane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banane.data_item().title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banane_s.to_pickle(banane.data_item().title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(banane.data_item().title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(banane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadData(\"Q3090425\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = testdata[0]\n",
    "b = testdata[1]\n",
    "pd.concat([a, b], axis=1, keys=[\"u\", \"v\"]).fillna(0).sum(axis=1, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = [n for n in os.listdir(\"pickles\") if n[0] == \"Q\"]\n",
    "testdata = [loadData(fn) for fn in fns]\n",
    "\n",
    "init = testdata[0]\n",
    "\n",
    "for td in testdata[1:]:\n",
    "    init = pd.concat([init, td], axis=1, keys=[\"l\", \"r\"]).fillna(0).sum(axis=1, level=1)\n",
    "    \n",
    "init.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init[\"Views\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init[\"Mentions\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init[\"Score\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pd.DataFrame(columns=[\"views\", \"mentions\", \"edits\"])\n",
    "cl.loc[495998] = [1, 2, 4]\n",
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(epfl_pages[:10]):\n",
    "    print(i)\n",
    "    getPageScore(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid, dat in df.groupby(\"userid\"):\n",
    "    print(uid)\n",
    "    for val, x in dat.groupby(\"minor\"):\n",
    "        print(val, sum(x[\"diff\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entry over time\n",
    "* list of incoming links\n",
    "* list of outgoing links\n",
    "* mention history\n",
    "\n",
    "work with user pages  \n",
    "potentially use the wikipedia package for smaller operations as it seems to be faster, maybe do some timed tests\n",
    "\n",
    "count improve counts by checking revisions 5 before and 5 after to ignore edit wars and such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to handle the problem or redirects and page name changes in the future.\n",
    "\n",
    "Will need to classify whether we found the subject through keywords or if EPFL was mentioned.\n",
    "\n",
    "We assume mentions are only added for performance and simplicity reasons. It's very rare that content will be removed from pages.\n",
    "\n",
    "Link pages with contributors\n",
    "\n",
    "Update data over time instead of recomputing everything\n",
    "\n",
    "Bot that changes names of EPFL or makes a suggestion on the talk page\n",
    "\n",
    "orcid, unique identifier for papers\n",
    "\n",
    "https://go.epfl.ch/wikiproject\n",
    "\n",
    "correlate growth of epfl mentions and wikipedia in general (or science related pages, lets see what we can do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r[\"size\"] for r in revs][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talkpage = pwb.Page(wiki_site, u\"Talk:Martin Vetterli\")\n",
    "talkpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(talkpage.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(page.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats[0].categoryinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that checks that the category records are correct (if a page has been removed from a category, it will put it in the legacy records instead)\n",
    "def sanitizeCategories():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epfl_pages[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extlink_data(page):\n",
    "    links = [link.split(\"/\", 5)[-1] if \"web.archive.org\" in link else link for link in page.extlinks()]\n",
    "    domains = pd.DataFrame([tldextract.extract(link) for link in links], columns=[\"subdomain\", \"domain\", \"suffix\"])\n",
    "    \n",
    "    try:\n",
    "        domains[\"tld\"] = domains[\"suffix\"].str.split(\".\", expand=False).apply(lambda e : e[-1])\n",
    "        domains[\"site\"] = domains[\"domain\"] + \".\" + domains[\"suffix\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sites = domains[\"site\"].value_counts()\n",
    "    tlds = domains[\"tld\"].value_counts()\n",
    "    return page.title(), sites, tlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, sites, tlds = extlink_data(page)\n",
    "tlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = epfl_pages[:500]\n",
    "\n",
    "series = np.array([extlink_data(t) for t in temp], dtype=object).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(s, name=t) for t, s in tuple(series[0:2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = pd.concat(series[1], axis=1, keys=series[0])\n",
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sum = refs.sum(axis=1)\n",
    "ref_sum[ref_sum > 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_cnt = refs.count(axis=1)\n",
    "ref_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(ref_sum, ref_cnt).axes\n",
    "ax.set_title(\"Scatterplot of domain repartition and frequency\")\n",
    "ax.set_xlabel(\"Number of occurences\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((9, 6))\n",
    "x_vals = np.linspace(0, 300)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (320, 280))\n",
    "for t in [\"epfl.ch\", \"unil.ch\", \"bbc.co.uk\", \"google.com\", \"nytimes.com\"]:\n",
    "    ax.scatter(ref_sum[t], ref_cnt[t], color=\"orange\")\n",
    "    ax.annotate(t, (ref_sum[t]+50, ref_cnt[t]+5), color=\"orange\", bbox=dict(facecolor='white', boxstyle=\"round,pad=0.3\"))\n",
    "ax.figure.savefig(os.path.join(im_path, \"domains.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_tld = pd.concat(series[2], axis=1, keys=series[0])\n",
    "refs_tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tld_sum = refs_tld.sum(axis=1)\n",
    "ref_tld_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tld_cnt = refs_tld.count(axis=1)\n",
    "ref_tld_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(ref_tld_sum, ref_tld_cnt).axes\n",
    "ax.set_title(\"Scatterplot of top-level-domain repartition and frequency\")\n",
    "ax.set_xlabel(\"Number of occurences\")\n",
    "ax.set_ylabel(\"Number of pages\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((9, 6))\n",
    "x_vals = np.linspace(0, 450)\n",
    "ax.plot(x_vals, x_vals)\n",
    "ax.annotate(\"y = x\", (720, 400))\n",
    "for t in [\"ch\", \"org\", \"com\", \"gov\"]:\n",
    "    ax.scatter(ref_tld_sum[t], ref_tld_cnt[t], color=\"orange\")\n",
    "    ax.annotate(t, (ref_tld_sum[t]+400, ref_tld_cnt[t]+1), color=\"orange\", bbox=dict(facecolor='white', boxstyle=\"round,pad=0.3\"))\n",
    "ax.figure.savefig(os.path.join(im_path, \"tlds.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "temp = pd.concat(series[2], axis=1, keys=series[0])\n",
    "sys.getsizeof(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = temp.astype(pd.SparseDtype(\"int\", np.nan))\n",
    "sys.getsizeof(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pszs = [getPageSize(p) for p in epfl_pages[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(pszs, axis=1)\n",
    "print(sys.getsizeof(df))\n",
    "\n",
    "print(sys.getsizeof(df.astype(pd.SparseDtype(\"int\", np.nan))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make page to item and item to page functions\n",
    "# Heavily bottlenecked by the `revisions` function unfortunately\n",
    "\n",
    "# Gets given keys of all revisions after a given timestamp\n",
    "def getRevisionsTags(page, ts, keys, content=False):\n",
    "    gen = page.revisions(reverse=True, content=content)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            v = next(gen)\n",
    "            t = v[\"timestamp\"]\n",
    "            \n",
    "            if ts is None or t > ts:\n",
    "                yield t, [v[key] for key in keys]\n",
    "            else:\n",
    "                return\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "def updatePageSizes(pagecodes, rescan=False):\n",
    "    path = os.path.join(\"pickles\", \"en_page_sizes.pkl\")\n",
    "    \n",
    "    try:\n",
    "        prev = pd.read_pickle(path)\n",
    "    except:\n",
    "        prev = pd.DataFrame()\n",
    "        \n",
    "    dfs = [prev]\n",
    "    \n",
    "    if rescan:\n",
    "        pagecodes += list(prev.columns)\n",
    "    \n",
    "    for code in tqdm(pagecodes):\n",
    "        try:\n",
    "            # Recover page from code\n",
    "            p = pwb.Page(wiki_site, pwb.ItemPage(repo, code).sitelinks[\"enwiki\"].ns_title())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Set limit timestamp (or None if no data yet)\n",
    "        ts = prev[code].last_valid_index() if code in prev.columns else None\n",
    "        \n",
    "        # Get values after that timestamp\n",
    "        values = [(t, v[0]) for t, v in getRevisionsTags(p, ts, [\"size\"])]\n",
    "        \n",
    "        if len(values):\n",
    "            # Sample every month and shift by 1 day to get 1st of month\n",
    "            df = pd.DataFrame(values, columns=[0, code]).set_index(0)\n",
    "            df = df.groupby(pd.Grouper(freq=\"1M\")).nth(-1).resample(\"1M\").pad()\n",
    "            df.index = df.index.shift(1, freq=\"D\")\n",
    "            \n",
    "            # Combine with old data if it exists\n",
    "            if code in prev.columns:\n",
    "                df = df.combine_first(prev[code])\n",
    "                \n",
    "\n",
    "            dfs.append(df)\n",
    "    \n",
    "    curr = pd.concat(dfs, axis=1)\n",
    "    curr = curr.ffill(axis=0)\n",
    "    curr.to_pickle(path)\n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcodes = [p.data_item().title() for p in epfl_pages[:300]]\n",
    "pszs = updatePageSizes(pcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pszs = pd.read_pickle(\"pickles/en_page_sizes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pszs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = pszs.sum(axis=1)\n",
    "total_size.index.name = \"Date\"\n",
    "total_size.name = \"Select Pages Size\"\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page count\n",
    "\n",
    "Here we look at how many of our pages exist at a given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageCounts(df):\n",
    "    ret = df.count(axis=1)\n",
    "    ret.index.name = \"Date\"\n",
    "    ret.name = \"Select Pages Count\"\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnts = pageCounts(pszs)\n",
    "pcnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page size and page counts ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(total_size / pcnts).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWikistats(fn, name, exp=0):\n",
    "    path = os.path.join(\"csv\", \"wikistats\", fn)\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    df.index = pd.to_datetime(df[\"month\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").rename(\"Date\")\n",
    "    df = df[\"total.total\"].rename(name) * 10**exp\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_size = loadWikistats(\"size_change_en.csv\", name=\"English Wikipedia Size\", exp=0)\n",
    "wiki_size = wiki_size[17:].cumsum()\n",
    "wiki_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_cnts = loadWikistats(\"pages_en.csv\", name=\"English Wikipedia Count\", exp=0)\n",
    "wiki_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_ratio = pd.concat([wiki_size, wiki_cnts], axis=1).dropna()\n",
    "wiki_ratio = wiki_ratio.iloc[:,0] / wiki_ratio.iloc[:,1]\n",
    "wiki_ratio.name = \"English Wikipedia Ratio\"\n",
    "wiki_ratio.plot()\n",
    "wiki_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_ratio = pd.concat([total_size, pcnts], axis=1).dropna()\n",
    "pages_ratio = pages_ratio.iloc[:,0] / pages_ratio.iloc[:,1]\n",
    "pages_ratio.name = \"Select Pages Ratio\"\n",
    "pages_ratio.plot()\n",
    "pages_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([total_size], axis=1).plot(logy=False, figsize=(9, 6))\n",
    "ax.set_title(\"Size in Bytes\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"size_comp_lin.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([total_size, wiki_size], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Size in Bytes (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"size_comp.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([pcnts, wiki_cnts], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Page counts (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"count_comp.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([pages_ratio, wiki_ratio], axis=1).plot(logy=True, figsize=(9, 6))\n",
    "ax.set_title(\"Size / Page Count Ratio (log scale)\")\n",
    "ax.figure.savefig(os.path.join(im_path, \"ratio_comp.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "page.revisions()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epfl_pages[100].title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only run on one page at a time\n",
    "start = time.time()\n",
    "revs = next(iter(api.PropertyGenerator('revisions', site=wiki_site, parameters={\n",
    "    'titles': 'Martin Vetterli',\n",
    "    'rvprop': 'timestamp|size',\n",
    "})))['revisions']\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize queries to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_data = []\n",
    "for p in epfl_pages[:100]:\n",
    "    mention_data.append(getMentions(p, epfl_alts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki-env",
   "language": "python",
   "name": "wiki-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
